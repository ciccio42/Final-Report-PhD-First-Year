\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\BKM@entry[2]{}
\bibstyle{ieeetr}
\babel@aux{american}{}
\BKM@entry{id=1,dest={636861707465722A2E33},srcline={5}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\newlabel{chapter:overview}{{}{2}{Overview}{chapter*.3}{}}
\@writefile{toc}{\contentsline {chapter}{Overview}{2}{chapter*.3}\protected@file@percent }
\BKM@entry{id=2,dest={636861707465722E31},srcline={1}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C30303064}
\BKM@entry{id=3,dest={73656374696F6E2E312E31},srcline={1}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{1}{3}{Background}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{3}{section.1.1}\protected@file@percent }
\newlabel{sec:intro}{{1.1}{3}{Introduction}{section.1.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:welding}{{1.1a}{3}{Robots involved in arc welding operation\relax }{figure.caption.4}{}}
\newlabel{sub@fig:welding}{{a}{3}{Robots involved in arc welding operation\relax }{figure.caption.4}{}}
\newlabel{fig:material_handling}{{1.1b}{3}{Robot involved in loading operation\relax }{figure.caption.4}{}}
\newlabel{sub@fig:material_handling}{{b}{3}{Robot involved in loading operation\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Industrial Robots: example of applications\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:industrial_robots_example}{{1.1}{3}{Industrial Robots: example of applications\relax }{figure.caption.4}{}}
\citation{pepper}
\citation{panda}
\citation{anne2021meta_learning_fast_adaptive}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{jang2022bc_z}
\citation{hafner2011reinforcement_in_feedback_controll}
\citation{sutton2018reinforcement}
\citation{argall2009robot_learning_from_demonstration}
\citation{kaelbling1996reinforcement_survey}
\citation{osa2018algorithmic}
\citation{james2013introduction_to_sl}
\citation{cortes1995support}
\citation{kullback1951information}
\citation{levine202rl_tutorial}
\citation{kumar2021orl_vs_bc}
\citation{mandlekar2022matters}
\citation{levine202rl_tutorial}
\citation{levine202rl_tutorial}
\citation{hussein2017imitation_learning_survey}
\newlabel{fig:onpolicy}{{1.2a}{5}{On-policy RL\relax }{figure.caption.5}{}}
\newlabel{sub@fig:onpolicy}{{a}{5}{On-policy RL\relax }{figure.caption.5}{}}
\newlabel{fig:offpolicy}{{1.2b}{5}{Off-policy RL\relax }{figure.caption.5}{}}
\newlabel{sub@fig:offpolicy}{{b}{5}{Off-policy RL\relax }{figure.caption.5}{}}
\newlabel{fig:offline}{{1.2c}{5}{Offline RL\relax }{figure.caption.5}{}}
\newlabel{sub@fig:offline}{{c}{5}{Offline RL\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Graphical Representation of Reinforcement Learning Methods \cite  {levine202rl_tutorial}\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:rl_methods}{{1.2}{5}{Graphical Representation of Reinforcement Learning Methods \cite {levine202rl_tutorial}\relax }{figure.caption.5}{}}
\citation{torabi2019recent_advances_lfo}
\citation{codevilla2018end_to_end}
\citation{zhang2018deep_vr_teleoperation}
\citation{maeda2017probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Imitation Learning: Taxonomy and main components\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:il_methods}{{1.3}{6}{Imitation Learning: Taxonomy and main components\relax }{figure.caption.6}{}}
\BKM@entry{id=4,dest={73656374696F6E2E312E32},srcline={1}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030655C3030302D5C3030306F5C303030665C3030302D5C303030745C303030685C303030655C3030302D5C303030415C303030725C30303074}
\BKM@entry{id=5,dest={73756273656374696F6E2E312E322E31},srcline={1}}{5C3337365C3337375C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D5C3030305C3034305C303030445C303030655C303030665C303030695C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}State-of-the-Art}{7}{section.1.2}\protected@file@percent }
\newlabel{sec:sota}{{1.2}{7}{State-of-the-Art}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Problem Definition}{7}{subsection.1.2.1}\protected@file@percent }
\newlabel{sec:problem_formulation}{{1.2.1}{7}{Problem Definition}{subsection.1.2.1}{}}
\citation{fang2019survey}
\citation{osa2018algorithmic}
\citation{kroemer2021review_robot_learning}
\BKM@entry{id=6,dest={73756273656374696F6E2E312E322E32},srcline={1}}{5C3337365C3337375C303030535C3030306F5C303030755C303030725C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C3030306D5C3030306F5C3030306E5C303030735C303030745C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\citation{fang2019survey}
\citation{johns2021coarse_to_fine}
\citation{johns2021coarse_to_fine}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{mandlekar2018roboturk}
\citation{mandlekar2019scaling}
\citation{mandlekar2022matters}
\citation{cyberglove}
\citation{touch}
\citation{mandlekar2018roboturk}
\citation{mandlekar2018roboturk}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Source of Demonstration}{9}{subsection.1.2.2}\protected@file@percent }
\newlabel{sec:source_of_demonstration}{{1.2.2}{9}{Source of Demonstration}{subsection.1.2.2}{}}
\newlabel{fig:kinesthetic}{{1.4a}{9}{Example of kinesthetic teaching \cite {johns2021coarse_to_fine}\relax }{figure.caption.7}{}}
\newlabel{sub@fig:kinesthetic}{{a}{9}{Example of kinesthetic teaching \cite {johns2021coarse_to_fine}\relax }{figure.caption.7}{}}
\newlabel{fig:teleoperation}{{1.4b}{9}{Example of teleoperation \cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.7}{}}
\newlabel{sub@fig:teleoperation}{{b}{9}{Example of teleoperation \cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Examples of Direct Demonstration\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:direct_demonstrations}{{1.4}{9}{Examples of Direct Demonstration\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Direct Demonstration}{9}{figure.caption.7}\protected@file@percent }
\citation{smith2019avid}
\citation{sermanet2018time_contrastive}
\citation{liu2019_mirroring_without_overimitation}
\citation{fang2019survey}
\citation{torabi2019recent_advances_lfo}
\citation{liu2017glove_force}
\BKM@entry{id=7,dest={73756273656374696F6E2E312E322E33},srcline={1}}{5C3337365C3337375C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\citation{osa2018algorithmic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces System diagram of Roboturk \cite  {mandlekar2018roboturk}\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:roboturk}{{1.5}{10}{System diagram of Roboturk \cite {mandlekar2018roboturk}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Indirect Demonstration}{10}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Methods}{10}{subsection.1.2.3}\protected@file@percent }
\newlabel{sec:methods}{{1.2.3}{10}{Methods}{subsection.1.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Behavioral Cloning (BC).}{10}{subsection.1.2.3}\protected@file@percent }
\citation{ijspeert2002learning}
\citation{ijspeert2013dynamical}
\citation{ijspeert2013dynamical}
\citation{meier2011movement_primitive}
\citation{caccavale2019kinesthetic}
\citation{agostini2020manipulation}
\citation{paraschos2013ProMPs}
\citation{pomerleau1988alvinn}
\citation{ross2010efficient_reductions}
\citation{ross2011dagger}
\citation{ross2011dagger}
\citation{laskey2017comparing_hc_rc}
\citation{ross2011dagger}
\citation{ross2011dagger}
\citation{kelly2019hg_dagger}
\citation{jang2022bc_z}
\citation{mandlekar2020human_in_the_loop}
\citation{chisari2022correct}
\citation{chisari2022correct}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Abstract Algorithm for Behavioral Cloning\relax }}{11}{algorithm.1}\protected@file@percent }
\newlabel{alg:bc}{{1}{11}{Abstract Algorithm for Behavioral Cloning\relax }{algorithm.1}{}}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{zhang2018deep_vr_teleoperation}
\citation{finn2017maml}
\citation{finn2017maml}
\citation{finn2017one_shot_visual_il}
\citation{yu2018daml}
\citation{yu2018one_shot_hil}
\citation{finn2017maml}
\citation{finn2017maml}
\citation{finn2017one_shot_visual_il}
\citation{duan2017one_shot_il}
\citation{duan2017one_shot_il}
\citation{yu2018daml}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces DAgger Algorithm \cite  {ross2011dagger}\relax }}{13}{algorithm.2}\protected@file@percent }
\newlabel{alg:dagger}{{2}{13}{DAgger Algorithm \cite {ross2011dagger}\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Architecture proposed in \cite  {zhang2018deep_vr_teleoperation}\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:deep_bc}{{1.6}{14}{Architecture proposed in \cite {zhang2018deep_vr_teleoperation}\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Statistics of Training set, and Test Success rate \cite  {zhang2018deep_vr_teleoperation}\relax }}{14}{table.caption.10}\protected@file@percent }
\newlabel{table:deep_vr_teleoperation_results}{{1.1}{14}{Statistics of Training set, and Test Success rate \cite {zhang2018deep_vr_teleoperation}\relax }{table.caption.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Model-Agnostic Meta-Learning (MAML) \cite  {finn2017maml}\relax }}{14}{algorithm.3}\protected@file@percent }
\newlabel{alg:maml}{{3}{14}{Model-Agnostic Meta-Learning (MAML) \cite {finn2017maml}\relax }{algorithm.3}{}}
\citation{yu2018daml}
\citation{yu2018daml}
\citation{james2018task_embedded}
\citation{bhutani2022attentive_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{mees2022calvin}
\citation{doasIcan2022}
\citation{mees2022hulc}
\citation{brohan2022rt}
\citation{shridhar2023perceiver}
\newlabel{eq:daml}{{1.4}{15}{Behavioral Cloning (BC)}{equation.1.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Tasks performed in \cite  {yu2018daml}. (Top row) Human demonstration, (Bottom row) robot demonstration. (Left) Placing task, (Middle) pushing task, (Right) pick-and-place task.\relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{fig:daml}{{1.7}{15}{Tasks performed in \cite {yu2018daml}. (Top row) Human demonstration, (Bottom row) robot demonstration. (Left) Placing task, (Middle) pushing task, (Right) pick-and-place task.\relax }{figure.caption.11}{}}
\citation{jiang2023vima}
\citation{james2018task_embedded}
\citation{bhutani2022attentive_one_shot}
\citation{james2018task_embedded}
\citation{james2018task_embedded}
\citation{bhutani2022attentive_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{yu2018daml}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Architecture proposed in \cite  {james2018task_embedded}\relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{fig:task_embedded}{{1.8}{16}{Architecture proposed in \cite {james2018task_embedded}\relax }{figure.caption.12}{}}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{yu2018daml}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces MOSAIC architecture \cite  {mandi2022towards_more_generalizable_one_shot}\relax }}{17}{figure.caption.13}\protected@file@percent }
\newlabel{fig:mosaic}{{1.9}{17}{MOSAIC architecture \cite {mandi2022towards_more_generalizable_one_shot}\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Results obtained in single-task and multi-task one-shot imitation learning \cite  {mandi2022towards_more_generalizable_one_shot}.\relax }}{17}{table.caption.15}\protected@file@percent }
\newlabel{table:mosaic}{{1.2}{17}{Results obtained in single-task and multi-task one-shot imitation learning \cite {mandi2022towards_more_generalizable_one_shot}.\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces MOSAIC \cite  {mandi2022towards_more_generalizable_one_shot} proposed tasks\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{fig:mosaic_tasks}{{1.10}{18}{MOSAIC \cite {mandi2022towards_more_generalizable_one_shot} proposed tasks\relax }{figure.caption.14}{}}
\citation{stepputtis2020language}
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{kelly2019hg_dagger}
\citation{jang2022bc_z}
\citation{jang2022bc_z}
\citation{brohan2022rt}
\citation{mees2022calvin}
\citation{mees2022hulc}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Architecture proposed in \cite  {stepputtis2020language}\relax }}{19}{figure.caption.16}\protected@file@percent }
\newlabel{fig:language_conditioned}{{1.11}{19}{Architecture proposed in \cite {stepputtis2020language}\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Set of object used in \cite  {stepputtis2020language} (left), sample of task execution (right)\relax }}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig:objects}{{1.12}{19}{Set of object used in \cite {stepputtis2020language} (left), sample of task execution (right)\relax }{figure.caption.17}{}}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{grill2003neural}
\citation{brohan2022rt}
\citation{brohan2022rt}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces Architecture proposed in \cite  {jang2022bc_z}\relax }}{20}{figure.caption.18}\protected@file@percent }
\newlabel{fig:bcz_architecture}{{1.13}{20}{Architecture proposed in \cite {jang2022bc_z}\relax }{figure.caption.18}{}}
\newlabel{fig:rt_1_dataset}{{1.14a}{20}{Examples of household scenarios in RT-1 large-scale dataset\relax }{figure.caption.19}{}}
\newlabel{sub@fig:rt_1_dataset}{{a}{20}{Examples of household scenarios in RT-1 large-scale dataset\relax }{figure.caption.19}{}}
\newlabel{fig:rt_1_model}{{1.14b}{20}{RT-1 Language-Conditioned Transformer based architecture proposed in \cite {brohan2022rt}\relax }{figure.caption.19}{}}
\newlabel{sub@fig:rt_1_model}{{b}{20}{RT-1 Language-Conditioned Transformer based architecture proposed in \cite {brohan2022rt}\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Household scenarios proposed in \cite  {brohan2022rt} (Figure \ref  {fig:rt_1_dataset}), architecture proposed in \cite  {brohan2022rt} (Figure \ref  {fig:rt_1_model})\relax }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:rt_1_dataset_and_architecture}{{1.14}{20}{Household scenarios proposed in \cite {brohan2022rt} (Figure \ref {fig:rt_1_dataset}), architecture proposed in \cite {brohan2022rt} (Figure \ref {fig:rt_1_model})\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Distribution of tasks in large-scale dataset proposed in \cite  {brohan2022rt}\relax }}{20}{table.caption.20}\protected@file@percent }
\newlabel{table:rf1_dataset}{{1.3}{20}{Distribution of tasks in large-scale dataset proposed in \cite {brohan2022rt}\relax }{table.caption.20}{}}
\citation{meier2011movement_primitive}
\citation{caccavale2019kinesthetic}
\citation{agostini2020manipulation}
\citation{caccavale2019kinesthetic}
\citation{xu2018neural_task_programming}
\citation{yang2015robot}
\citation{yu2018one_shot_hil}
\citation{Mandlekar2020GTI}
\citation{yu2018one_shot_hil}
\citation{Mandlekar2020GTI}
\newlabel{table:rt1_results}{{1.4}{21}{Behavioral Cloning (BC)}{table.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.5}{\ignorespaces Results reported in \cite  {brohan2022rt} by training the same model RT-1 with different dataset size\relax }}{21}{table.caption.21}\protected@file@percent }
\citation{grimes2009learning_actions_through_imitation}
\citation{englert2013probabilistic}
\citation{deisenroth2014multi_task}
\citation{abbeel2004apprenticeship}
\citation{ratliff2006maximum_margin}
\citation{ziebart2008maximum_entropy}
\citation{ratliff2009learning_to_search}
\citation{wulfmeier2015deep_inverse_rl}
\citation{finn2016guided_cost_learning}
\citation{ratliff2006maximum_margin}
\citation{ratliff2009learning_to_search}
\citation{ziebart2008maximum_entropy}
\citation{wulfmeier2015deep_inverse_rl}
\citation{finn2016guided_cost_learning}
\citation{ziebart2008maximum_entropy}
\citation{wulfmeier2015deep_inverse_rl}
\citation{finn2016guided_cost_learning}
\citation{finn2016guided_cost_learning}
\citation{finn2016guided_cost_learning}
\citation{finn2016guided_cost_learning}
\citation{levine2014lqr_flm}
\citation{levine2014lqr_flm}
\citation{das2021model_based_irl_from_vd}
\citation{das2021model_based_irl_from_vd}
\citation{das2021model_based_irl_from_vd}
\citation{das2021model_based_irl_from_vd}
\@writefile{toc}{\contentsline {paragraph}{Inverse Reinforcement Learning (IRL)}{23}{Item.32}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Classic feature matching IRL algorithm\relax }}{23}{algorithm.4}\protected@file@percent }
\newlabel{alg:irl}{{4}{23}{Classic feature matching IRL algorithm\relax }{algorithm.4}{}}
\newlabel{lqr}{{1.2.3}{24}{Inverse Reinforcement Learning (IRL)}{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Guided-Cost-Learning Algorithm \cite  {finn2016guided_cost_learning}\relax }}{24}{algorithm.5}\protected@file@percent }
\newlabel{alg:guided_cost_learning}{{5}{24}{Guided-Cost-Learning Algorithm \cite {finn2016guided_cost_learning}\relax }{algorithm.5}{}}
\citation{ho2016gail}
\citation{ho2016gail}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Architecture proposed in \cite  {das2021model_based_irl_from_vd}\relax }}{25}{figure.caption.22}\protected@file@percent }
\newlabel{fig:model_based_irl}{{1.15}{25}{Architecture proposed in \cite {das2021model_based_irl_from_vd}\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Generative Adversarial Imitation Learning (GAIL)}{25}{figure.caption.22}\protected@file@percent }
\newlabel{para:gail}{{1.2.3}{25}{Generative Adversarial Imitation Learning (GAIL)}{figure.caption.22}{}}
\newlabel{formula:regularized_max_ent}{{1.5}{25}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.5}{}}
\citation{ziebart2008maximum_entropy}
\citation{kostrikov2018discriminator}
\citation{fu2018airl}
\citation{ghasemipour2020divergence_minimization_perspective}
\citation{schulman2015trpo}
\citation{brockman2016openai}
\citation{liu2018imitation_from_observation}
\citation{reddy2019sqil}
\citation{zolna2021task_relevant_ail}
\citation{rafailov2021visual_ail}
\citation{zolna2021task_relevant_ail}
\citation{rafailov2021visual_ail}
\citation{zolna2021task_relevant_ail}
\citation{zolna2021task_relevant_ail}
\citation{barth2018d4pg}
\citation{ho2016gail}
\citation{zolna2021task_relevant_ail}
\citation{zolna2021task_relevant_ail}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Generative Adversarial Imitation Learning Algorithm\relax }}{26}{algorithm.6}\protected@file@percent }
\newlabel{alg:gail}{{6}{26}{Generative Adversarial Imitation Learning Algorithm\relax }{algorithm.6}{}}
\newlabel{formula:policy_characterization}{{1.6}{26}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.6}{}}
\newlabel{formula:ga_regularization}{{1.7}{26}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.7}{}}
\newlabel{formula:ga_regularizer_conjugate}{{1.8}{26}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.8}{}}
\citation{rafailov2021visual_ail}
\citation{reddy2019sqil}
\citation{kostrikov2018discriminator}
\citation{rafailov2021visual_ail}
\citation{rafailov2021visual_ail}
\citation{smith2019avid}
\citation{xiong2021learning_by_watching}
\citation{li2021meta_watching_video_demonstration}
\citation{zakka2022xirl}
\citation{dwibedi2019tcc}
\citation{smith2019avid}
\citation{li2021meta_watching_video_demonstration}
\citation{zhu2017cycle_gan}
\citation{xiong2021learning_by_watching}
\citation{huang2018munit}
\citation{zakka2022xirl}
\citation{zakka2022xirl}
\citation{zakka2022xirl}
\citation{sermanet2018time_contrastive}
\citation{liu2018imitation_from_observation}
\citation{sermanet2018time_contrastive}
\citation{schroff2015triplet_loss}
\citation{liu2018imitation_from_observation}
\citation{sermanet2018time_contrastive}
\citation{sermanet2018time_contrastive}
\citation{liu2018imitation_from_observation}
\citation{liu2018imitation_from_observation}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Experimental results on tasks without and with spurious features \cite  {zolna2021task_relevant_ail}\relax }}{27}{figure.caption.23}\protected@file@percent }
\newlabel{fig:trail_results}{{1.16}{27}{Experimental results on tasks without and with spurious features \cite {zolna2021task_relevant_ail}\relax }{figure.caption.23}{}}
\newlabel{formula:elbo}{{1.9}{27}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.9}{}}
\newlabel{formula:discriminator}{{1.10}{27}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.10}{}}
\newlabel{formula:value_function}{{1.11}{27}{Generative Adversarial Imitation Learning (GAIL)}{equation.1.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Control tasks solved in \cite  {rafailov2021visual_ail}. From left to right: cheetah run, walker walk, car racing, claw rotate, baoding balls\relax }}{28}{figure.caption.24}\protected@file@percent }
\newlabel{fig:vmail}{{1.17}{28}{Control tasks solved in \cite {rafailov2021visual_ail}. From left to right: cheetah run, walker walk, car racing, claw rotate, baoding balls\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning from Observation (LfO)}{28}{figure.caption.24}\protected@file@percent }
\newlabel{sec:lfo}{{1.2.3}{28}{Learning from Observation (LfO)}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Representation of embodiment mismatch problem. (Left) The source domain represented by a video of human performing a task. (Right) The target domain, represented by the robot that executes the observed task.\relax }}{28}{figure.caption.25}\protected@file@percent }
\newlabel{fig:embodiment}{{1.18}{28}{Representation of embodiment mismatch problem. (Left) The source domain represented by a video of human performing a task. (Right) The target domain, represented by the robot that executes the observed task.\relax }{figure.caption.25}{}}
\citation{liu2018imitation_from_observation}
\citation{sermanet2018time_contrastive}
\citation{xiong2021learning_by_watching}
\citation{zakka2022xirl}
\citation{liu2018imitation_from_observation}
\citation{sermanet2018time_contrastive}
\citation{xiong2021learning_by_watching}
\citation{zakka2022xirl}
\citation{sermanet2018time_contrastive}
\citation{merel2017learning}
\citation{torabi2018gaifo}
\citation{merel2017learning}
\citation{torabi2018gaifo}
\citation{torabi2018gaifo}
\citation{torabi2018gaifo}
\citation{ho2016gail}
\citation{brockman2016openai}
\citation{sermanet2018time_contrastive}
\citation{torabi2018bco}
\citation{schulman2015trpo}
\citation{torabi2021dealio}
\citation{chebotar2017pilqr}
\citation{torabi2018gaifo}
\citation{torabi2018gaifo}
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces Temporal-Cycle Consistency representation, used to learn an embodiment-agnostic encoder in \cite  {zakka2022xirl}\relax }}{29}{figure.caption.26}\protected@file@percent }
\newlabel{fig:xirl}{{1.19}{29}{Temporal-Cycle Consistency representation, used to learn an embodiment-agnostic encoder in \cite {zakka2022xirl}\relax }{figure.caption.26}{}}
\newlabel{fig:time_contrastive}{{1.20a}{30}{Time-Contrastive network, proposed in \cite {sermanet2018time_contrastive}.\relax }{figure.caption.27}{}}
\newlabel{sub@fig:time_contrastive}{{a}{30}{Time-Contrastive network, proposed in \cite {sermanet2018time_contrastive}.\relax }{figure.caption.27}{}}
\newlabel{fig:context-translation}{{1.20b}{30}{Context-Translation network, proposed in \cite {liu2018imitation_from_observation}\relax }{figure.caption.27}{}}
\newlabel{sub@fig:context-translation}{{b}{30}{Context-Translation network, proposed in \cite {liu2018imitation_from_observation}\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Examples of how the mismatch between demonstrator viewpoint and learner viewpoint can be handled.\relax }}{30}{figure.caption.27}\protected@file@percent }
\newlabel{fig:differet_viewpoint}{{1.20}{30}{Examples of how the mismatch between demonstrator viewpoint and learner viewpoint can be handled.\relax }{figure.caption.27}{}}
\citation{nair2017combining}
\citation{torabi2018bco}
\citation{guo2019hybrid_rl}
\citation{radosavovic2021state_only_demo}
\citation{nair2017combining}
\citation{torabi2018bco}
\citation{guo2019hybrid_rl}
\citation{torabi2018bco}
\citation{mnih2016a2c}
\citation{radosavovic2021state_only_demo}
\citation{Rajeswaran18_learning_complex_dexterous}
\citation{Rajeswaran18_learning_complex_dexterous}
\citation{torabi2018bco}
\citation{torabi2018bco}
\citation{smith2019avid}
\citation{torabi2021dealio}
\citation{smith2019avid}
\citation{zhang2019solar}
\citation{Kingma2014_vae}
\citation{levine2014lqr_flm}
\citation{sermanet2018time_contrastive}
\citation{torabi2018bco}
\citation{torabi2021dealio}
\citation{torabi2018gaifo}
\citation{chebotar2017pilqr}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces GAIfO algorithm \cite  {torabi2018gaifo}\relax }}{31}{algorithm.7}\protected@file@percent }
\newlabel{alg:gaifo_algorithm}{{7}{31}{GAIfO algorithm \cite {torabi2018gaifo}\relax }{algorithm.7}{}}
\newlabel{fig:low_dimensional}{{1.21a}{32}{Experimental results in low-dimensional state space\relax }{figure.caption.28}{}}
\newlabel{sub@fig:low_dimensional}{{a}{32}{Experimental results in low-dimensional state space\relax }{figure.caption.28}{}}
\newlabel{fig:high_dimensional}{{1.21b}{32}{Experimental results in high-dimensional state space\relax }{figure.caption.28}{}}
\newlabel{sub@fig:high_dimensional}{{b}{32}{Experimental results in high-dimensional state space\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces Experimental results reported in \cite  {torabi2018gaifo}.\relax }}{32}{figure.caption.28}\protected@file@percent }
\newlabel{fig:gaifo_results}{{1.21}{32}{Experimental results reported in \cite {torabi2018gaifo}.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Representation of the learning procedure proposed by \cite  {torabi2018bco}\relax }}{33}{figure.caption.29}\protected@file@percent }
\newlabel{fig:bco}{{1.22}{33}{Representation of the learning procedure proposed by \cite {torabi2018bco}\relax }{figure.caption.29}{}}
\newlabel{formula:linear_gaussian_controller}{{1.12}{33}{Learning from Observation (LfO)}{equation.1.2.12}{}}
\newlabel{formula:quadratic_cost_function}{{1.13}{33}{Learning from Observation (LfO)}{equation.1.2.13}{}}
\newlabel{formula:gaussian_dyn}{{1.14}{33}{Learning from Observation (LfO)}{equation.1.2.14}{}}
\citation{torabi2021dealio}
\citation{torabi2021dealio}
\citation{torabi2021dealio}
\citation{torabi2018gaifo}
\citation{torabi2021dealio}
\citation{torabi2018gaifo}
\BKM@entry{id=8,dest={73756273656374696F6E2E312E322E34},srcline={1}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C30303079}
\newlabel{formula:output_discriminator}{{1.15}{34}{Learning from Observation (LfO)}{equation.1.2.15}{}}
\newlabel{fig:dealio_task}{{1.23a}{34}{Control Tasks solved in \cite {torabi2021dealio}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:dealio_task}{{a}{34}{Control Tasks solved in \cite {torabi2021dealio}\relax }{figure.caption.30}{}}
\newlabel{fig:dealio_performance}{{1.23b}{34}{Performance of DEALIO \cite {torabi2021dealio} compared against GAIfO \cite {torabi2018gaifo}, with respect to the number of trajectories sampled during the learning process.\relax }{figure.caption.30}{}}
\newlabel{sub@fig:dealio_performance}{{b}{34}{Performance of DEALIO \cite {torabi2021dealio} compared against GAIfO \cite {torabi2018gaifo}, with respect to the number of trajectories sampled during the learning process.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces DEAILO: (\ref  {fig:dealio_task}) Control Tasks, (\ref  {fig:dealio_performance}) Performance Level\relax }}{34}{figure.caption.30}\protected@file@percent }
\newlabel{fig:dealio}{{1.23}{34}{DEAILO: (\ref {fig:dealio_task}) Control Tasks, (\ref {fig:dealio_performance}) Performance Level\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Summary}{35}{subsection.1.2.4}\protected@file@percent }
\newlabel{sec:summary}{{1.2.4}{35}{Summary}{subsection.1.2.4}{}}
\BKM@entry{id=9,dest={636861707465722E32},srcline={1}}{5C3337365C3337375C303030525C303030655C303030735C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030505C3030306C5C303030615C3030306E}
\BKM@entry{id=10,dest={73656374696F6E2E322E31},srcline={1}}{5C3337365C3337375C303030525C303030655C303030735C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030545C3030306F5C303030705C303030695C30303063}
\citation{finn2017one_shot_visual_il}
\citation{yu2018one_shot_hil}
\citation{yu2018daml}
\citation{jang2022bc_z}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{jang2022bc_z}
\citation{yu2018daml}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Research Plan}{36}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:research_plan}{{2}{36}{Research Plan}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Research Topic}{36}{section.2.1}\protected@file@percent }
\newlabel{sec:research_topic}{{2.1}{36}{Research Topic}{section.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\textit  {Reason-then-act: How the separation between perception and action can affect the performance of an Imitation-Learning System}}{37}{section.2.1}\protected@file@percent }
\citation{stepputtis2020language}
\citation{jang2022bc_z}
\citation{brohan2022rt}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{james2018task_embedded}
\citation{stepputtis2020language}
\citation{bhutani2022attentive_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{brohan2022rt}
\citation{brohan2022rt}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{brohan2022rt}
\citation{zhang2018deep_vr_teleoperation}
\citation{duan2017one_shot_il}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{brohan2022rt}
\BKM@entry{id=11,dest={73656374696F6E2E322E32},srcline={1}}{5C3337365C3337375C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030525C303030655C303030735C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030695C303030745C30303079}
\citation{park2021object}
\citation{belkhale2023plato}
\citation{zhu2023viola}
\citation{jiang2023vima}
\citation{belkhale2023plato}
\citation{zhu2023viola}
\citation{jiang2023vima}
\citation{he2017mask}
\citation{perez2018film}
\citation{perez2018film}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Proposed Research Activity}{39}{section.2.2}\protected@file@percent }
\newlabel{sec:research_activity}{{2.2}{39}{Proposed Research Activity}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textit  {Vision Question and Answering task}, starting from a an image and a text, the method is able to generate an answer by focusing on specific part of the image based on the question.\relax }}{40}{figure.caption.31}\protected@file@percent }
\newlabel{fig:film_attention}{{2.1}{40}{\textit {Vision Question and Answering task}, starting from a an image and a text, the method is able to generate an answer by focusing on specific part of the image based on the question.\relax }{figure.caption.31}{}}
\BKM@entry{id=12,dest={636861707465722E33},srcline={1}}{5C3337365C3337375C303030505C303030725C303030655C3030306C5C303030695C3030306D5C303030695C3030306E5C303030615C303030725C303030795C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=13,dest={73656374696F6E2E332E31},srcline={1}}{5C3337365C3337375C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030725C303030695C303030705C303030745C303030695C3030306F5C3030306E}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{dasari2021transformers_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\citation{mandi2022towards_more_generalizable_one_shot}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Preliminary Results}{42}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:preliminary_results}{{3}{42}{Preliminary Results}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset Description}{42}{section.3.1}\protected@file@percent }
\newlabel{sec:dataset_description}{{3.1}{42}{Dataset Description}{section.3.1}{}}
\BKM@entry{id=14,dest={73656374696F6E2E332E32},srcline={1}}{5C3337365C3337375C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Reference dataset's cardinality\relax }}{43}{table.caption.32}\protected@file@percent }
\newlabel{table:reference_dataset_cardinality}{{3.1}{43}{Reference dataset's cardinality\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Experiments Results}{43}{section.3.2}\protected@file@percent }
\newlabel{sec:experiment_results}{{3.2}{43}{Experiments Results}{section.3.2}{}}
\newlabel{fig:first_variation}{{3.1a}{44}{First variation, green box into first bin\relax }{figure.caption.33}{}}
\newlabel{sub@fig:first_variation}{{a}{44}{First variation, green box into first bin\relax }{figure.caption.33}{}}
\newlabel{fig:second_variation}{{3.1b}{44}{Second variation, green box into second bin\relax }{figure.caption.33}{}}
\newlabel{sub@fig:second_variation}{{b}{44}{Second variation, green box into second bin\relax }{figure.caption.33}{}}
\newlabel{fig:third_variation}{{3.1c}{44}{Third variation, green box into third bin\relax }{figure.caption.33}{}}
\newlabel{sub@fig:third_variation}{{c}{44}{Third variation, green box into third bin\relax }{figure.caption.33}{}}
\newlabel{fig:fourth_variation}{{3.1d}{44}{Fourth variation, green box into fourth bin\relax }{figure.caption.33}{}}
\newlabel{sub@fig:fourth_variation}{{d}{44}{Fourth variation, green box into fourth bin\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Example of variations for the Pick and Place task. The same set of variations is repeated for each block\relax }}{44}{figure.caption.33}\protected@file@percent }
\newlabel{fig:example_of_variations_for_pick_place}{{3.1}{44}{Example of variations for the Pick and Place task. The same set of variations is repeated for each block\relax }{figure.caption.33}{}}
\newlabel{fig:first_variation_nut}{{3.2a}{44}{First variation, assembly gray nut with right peg\relax }{figure.caption.34}{}}
\newlabel{sub@fig:first_variation_nut}{{a}{44}{First variation, assembly gray nut with right peg\relax }{figure.caption.34}{}}
\newlabel{fig:second_variation_nut}{{3.2b}{44}{Second variation, assembly gray nut with middle peg\relax }{figure.caption.34}{}}
\newlabel{sub@fig:second_variation_nut}{{b}{44}{Second variation, assembly gray nut with middle peg\relax }{figure.caption.34}{}}
\newlabel{fig:third_variation_nut}{{3.2c}{44}{Third variation, assembly gray nut with left peg\relax }{figure.caption.34}{}}
\newlabel{sub@fig:third_variation_nut}{{c}{44}{Third variation, assembly gray nut with left peg\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example of variations for the Nut-Assembly. The same set of variations is repeated for each nut\relax }}{44}{figure.caption.34}\protected@file@percent }
\newlabel{fig:examples_of_variations_for_nut_assembly}{{3.2}{44}{Example of variations for the Nut-Assembly. The same set of variations is repeated for each nut\relax }{figure.caption.34}{}}
\newlabel{fig:pick_place_first_variation}{{3.3a}{45}{Trajectory distribution along the x-y-z axes for the first Pick-Place variation\relax }{figure.caption.35}{}}
\newlabel{sub@fig:pick_place_first_variation}{{a}{45}{Trajectory distribution along the x-y-z axes for the first Pick-Place variation\relax }{figure.caption.35}{}}
\newlabel{fig:nut_assembly_first_variation}{{3.3b}{45}{Trajectory distribution along the x-y-z axes for the first Nut-Assembly variation\relax }{figure.caption.35}{}}
\newlabel{sub@fig:nut_assembly_first_variation}{{b}{45}{Trajectory distribution along the x-y-z axes for the first Nut-Assembly variation\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Trajectory Distribution along the x-y-z axes for the first variation of Pick-Place (\ref  {fig:pick_place_first_variation}) and Nut-Assembly (\ref  {fig:nut_assembly_first_variation}) task\relax }}{45}{figure.caption.35}\protected@file@percent }
\newlabel{fig:dataset_distribution}{{3.3}{45}{Trajectory Distribution along the x-y-z axes for the first variation of Pick-Place (\ref {fig:pick_place_first_variation}) and Nut-Assembly (\ref {fig:nut_assembly_first_variation}) task\relax }{figure.caption.35}{}}
\BKM@entry{id=15,dest={636861707465722E34},srcline={1}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030695C303030745C303030695C303030655C30303073}
\BKM@entry{id=16,dest={73656374696F6E2E342E31},srcline={1}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030645C303030655C303030645C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C30303073}
\BKM@entry{id=17,dest={73756273656374696F6E2E342E312E31},srcline={1}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C3030306C5C303030735C303030615C303030725C303030795C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C30303073}
\BKM@entry{id=18,dest={73656374696F6E2E342E32},srcline={1}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030695C303030745C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Other Activities}{46}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:other_activities}{{4}{46}{Other Activities}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Attended Courses}{46}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Compulsary Courses}{46}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Other Activities}{47}{section.4.2}\protected@file@percent }
\BKM@entry{id=19,dest={73656374696F6E2E342E32},srcline={133}}{5C3337365C3337375C303030425C303030695C303030625C3030306C5C303030695C3030306F5C303030675C303030725C303030615C303030705C303030685C30303079}
\bibdata{bibliography.bib}
\bibcite{pepper}{1}
\bibcite{panda}{2}
\bibcite{anne2021meta_learning_fast_adaptive}{3}
\bibcite{mandi2022towards_more_generalizable_one_shot}{4}
\bibcite{jang2022bc_z}{5}
\bibcite{hafner2011reinforcement_in_feedback_controll}{6}
\bibcite{sutton2018reinforcement}{7}
\bibcite{argall2009robot_learning_from_demonstration}{8}
\bibcite{kaelbling1996reinforcement_survey}{9}
\bibcite{osa2018algorithmic}{10}
\bibcite{james2013introduction_to_sl}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{48}{section.4.2}\protected@file@percent }
\bibcite{cortes1995support}{12}
\bibcite{kullback1951information}{13}
\bibcite{levine202rl_tutorial}{14}
\bibcite{kumar2021orl_vs_bc}{15}
\bibcite{mandlekar2022matters}{16}
\bibcite{hussein2017imitation_learning_survey}{17}
\bibcite{torabi2019recent_advances_lfo}{18}
\bibcite{codevilla2018end_to_end}{19}
\bibcite{zhang2018deep_vr_teleoperation}{20}
\bibcite{maeda2017probabilistic}{21}
\bibcite{fang2019survey}{22}
\bibcite{kroemer2021review_robot_learning}{23}
\bibcite{johns2021coarse_to_fine}{24}
\bibcite{mandlekar2018roboturk}{25}
\bibcite{mandlekar2019scaling}{26}
\bibcite{cyberglove}{27}
\bibcite{touch}{28}
\bibcite{smith2019avid}{29}
\bibcite{sermanet2018time_contrastive}{30}
\bibcite{liu2019_mirroring_without_overimitation}{31}
\bibcite{liu2017glove_force}{32}
\bibcite{ijspeert2002learning}{33}
\bibcite{ijspeert2013dynamical}{34}
\bibcite{meier2011movement_primitive}{35}
\bibcite{caccavale2019kinesthetic}{36}
\bibcite{agostini2020manipulation}{37}
\bibcite{paraschos2013ProMPs}{38}
\bibcite{pomerleau1988alvinn}{39}
\bibcite{ross2010efficient_reductions}{40}
\bibcite{ross2011dagger}{41}
\bibcite{laskey2017comparing_hc_rc}{42}
\bibcite{kelly2019hg_dagger}{43}
\bibcite{mandlekar2020human_in_the_loop}{44}
\bibcite{chisari2022correct}{45}
\bibcite{finn2017maml}{46}
\bibcite{finn2017one_shot_visual_il}{47}
\bibcite{yu2018daml}{48}
\bibcite{yu2018one_shot_hil}{49}
\bibcite{duan2017one_shot_il}{50}
\bibcite{james2018task_embedded}{51}
\bibcite{bhutani2022attentive_one_shot}{52}
\bibcite{dasari2021transformers_one_shot}{53}
\bibcite{stepputtis2020language}{54}
\bibcite{mees2022calvin}{55}
\bibcite{doasIcan2022}{56}
\bibcite{mees2022hulc}{57}
\bibcite{brohan2022rt}{58}
\bibcite{shridhar2023perceiver}{59}
\bibcite{jiang2023vima}{60}
\bibcite{grill2003neural}{61}
\bibcite{xu2018neural_task_programming}{62}
\bibcite{yang2015robot}{63}
\bibcite{Mandlekar2020GTI}{64}
\bibcite{grimes2009learning_actions_through_imitation}{65}
\bibcite{englert2013probabilistic}{66}
\bibcite{deisenroth2014multi_task}{67}
\bibcite{abbeel2004apprenticeship}{68}
\bibcite{ratliff2006maximum_margin}{69}
\bibcite{ziebart2008maximum_entropy}{70}
\bibcite{ratliff2009learning_to_search}{71}
\bibcite{wulfmeier2015deep_inverse_rl}{72}
\bibcite{finn2016guided_cost_learning}{73}
\bibcite{levine2014lqr_flm}{74}
\bibcite{das2021model_based_irl_from_vd}{75}
\bibcite{ho2016gail}{76}
\bibcite{kostrikov2018discriminator}{77}
\bibcite{fu2018airl}{78}
\bibcite{ghasemipour2020divergence_minimization_perspective}{79}
\bibcite{schulman2015trpo}{80}
\bibcite{brockman2016openai}{81}
\bibcite{liu2018imitation_from_observation}{82}
\bibcite{reddy2019sqil}{83}
\bibcite{zolna2021task_relevant_ail}{84}
\bibcite{rafailov2021visual_ail}{85}
\bibcite{barth2018d4pg}{86}
\bibcite{xiong2021learning_by_watching}{87}
\bibcite{li2021meta_watching_video_demonstration}{88}
\bibcite{zakka2022xirl}{89}
\bibcite{dwibedi2019tcc}{90}
\bibcite{zhu2017cycle_gan}{91}
\bibcite{huang2018munit}{92}
\bibcite{schroff2015triplet_loss}{93}
\bibcite{merel2017learning}{94}
\bibcite{torabi2018gaifo}{95}
\bibcite{torabi2018bco}{96}
\bibcite{torabi2021dealio}{97}
\bibcite{chebotar2017pilqr}{98}
\bibcite{nair2017combining}{99}
\bibcite{guo2019hybrid_rl}{100}
\bibcite{radosavovic2021state_only_demo}{101}
\bibcite{mnih2016a2c}{102}
\bibcite{Rajeswaran18_learning_complex_dexterous}{103}
\bibcite{zhang2019solar}{104}
\bibcite{Kingma2014_vae}{105}
\bibcite{park2021object}{106}
\bibcite{belkhale2023plato}{107}
\bibcite{zhu2023viola}{108}
\bibcite{he2017mask}{109}
\bibcite{perez2018film}{110}
\gdef \@abspage@last{59}
