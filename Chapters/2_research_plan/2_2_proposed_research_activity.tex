\section{Proposed Research Activity}
\label{sec:research_activity}
After the presentation of the context of interest and the aspects to be treated in the current research proposal, this section will formalize the improvement hypotheses reported in Section \ref{sec:research_topic}, identifying the main methods of interest from which will start the application of the proposed changes, and the procedures that will be used to validate and compare the proposed approach against the state-of-the-art methods.
Starting from the discussion made in Section \ref{sec:research_topic}, one consideration arises. Given a scenario where the robot must execute a task given a command, how to model a system that is able to identify the objects of interest, which may change based on the command.
In literature, there are recent \textit{Imitation Learning methods}, that follow an object-oriented paradigm \cite{park2021object, belkhale2023plato, zhu2023viola, jiang2023vima}. However, methods like \cite{belkhale2023plato} primarily focus on learning the affordance of individual objects. These methods take as input the current observation and the desired state of a specific object, typically in single-object scenarios. On the other hand, techniques in the field of Vision-Based Manipulation, such as \cite{zhu2023viola, jiang2023vima}, leverage pre-trained object detectors like Mask-RCNN \cite{he2017mask}. These detectors identify the top-k predicted bounding-boxes that \textbf{potentially contain objects} in the scene and utilize this information to infer the appropriate actions. In contrast to these methods, our approach draws inspiration from Computer Vision tasks like 'Vision Question and Answering' \cite{perez2018film}. In this task, the system is responsible for generating an answer to a given query (e.g., a textual representation of a question) based on an input image. What distinguishes this approach is its capability to \textbf{selectively focus on specific segments of the image}, guided by the content of the question, as illustrated in Figure \ref{fig:film_attention}
\input{Figures/film_attention_mechanism.tex}
In conclusion, our research focuses on evaluating the adaptability of visual-scene reasoning modules, as exemplified by \cite{perez2018film}, to address a novel problem termed \textit{`Conditioned-Target Object Detector'}. The primary objective is to develop a system capable of precisely identifying a the \textit{\textbf{target object}} based on the desired task. This kind of problem can be formulated as an \textbf{\textit{Object Detection Problem}}, as it will be formulated in Chapter \ref{chapter:preliminary_results}. Moreover, in a multi-task context, such a system needs to be \textit{\textbf{object-class agnostic}}, meaning it should have the ability to predict both box-shaped target objects and nut-shaped target objects without any prior knowledge about the object's class.
In order to validate the hypothesis that following a modular approach can affect positively the system performance, we will follow an incremental approach, divided into 4 main steps:
\begin{itemize}
    \item \textbf{Baseline Definition}: This initial step is crucial to establish a baseline and pinpoint critical aspects of the current state-of-the-art methods
    \item \textbf{Incorporate Ground-Truth Information into the Baseline's Control Module}: After assessing the baseline, the second phase focuses on empirically assessing whether adding target-object information to the control modules can enhance system performance. During this phase, to prevent error propagation, the control module will receive input based on ground-truth information.
    \item \textbf{ Development of a Conditioned-Target Object Detector System}: Once the second step is completed, we can formulate the Conditioned-Target Object Detector problem and evaluate its efficacy in addressing the tasks at hand.
    \item \textbf{Integration of the Developed System with the Baseline}: After achieving a stable system in the previous step, we will integrate the developed system with the baseline architecture to assess overall system performance
\end{itemize}
In the initial iteration, the methods will undergo training in a Single-Task Multi-Variation scenario. Subsequently, leveraging the class-agnostic property, the system will be extended to a Multi-Task Multi-Variation setup.
Results and additional details concerning these problems will be presented in Chapter \ref{chapter:preliminary_results}."
% \newline As discussed in Section \ref{sec:research_topic}, it was seen that current methods start from a representation of the environment that lacks geometric information and that is obtained from the point of view different from that of the robot, and in particular from that of the end-effector. 
% Concerning this point, starting from the state-of-the-art architecture \cite{mandi2022towards_more_generalizable_one_shot} based on Transformer, the goal will be to extend it in such a way that it will be able to handle time sequences characterized by RGB-D observations, and taken from a camera mounted on the end-effector. Several approaches can be used to achieve this. In fact, in the Computer Vision community, RGB-D images can be handled in different ways. For example, one can apply convolutional layers separately on the RGB and depth images, as in \cite{zhang2018deep_vr_teleoperation}, and subsequently perform a fusion between the extracted features based, for example, on their concatenation. Another approach may be using a 3D Point-Cloud Neural Network, such as \cite{qi2017pointnet++} used in \cite{fang2020graspnet}. To answer the question which of the two approaches to prefer, given that the reference method \cite{mandi2022towards_more_generalizable_one_shot} has a transformer-based architecture, a first step may be to apply convolutional filters separately on the RGB and D images and then combine them appropriately, going on to evaluate different fusion techniques, starting from simple concatenation to the use of a weighted sum, with the ultimate goal of generating the embedding as input to the transformer.
% \newline This setting will allow evaluating the hypothesis that adding geometric information can reduce failures caused by collisions with objects of interest or by the inability to determine when to close or open the gripper.  
% The resulting method will then be tested and compared first in simulation, taking advantage of the open-source simulation environments \cite{brockman2016openai,zhu2020robosuite}. Then, based on the obtained results, the proposed architecture will be tested and evaluated on a real robot platform, adding an additional experimental contribution that current methods such as \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot} lack.
% \newline As for the tasks of interest, the main focus will be devoted to industrial tasks (e.g., pick-and-place, push, peg-insertion), introducing an additional constraint with respect to \cite{mandi2022towards_more_generalizable_one_shot}, since the system will have to be able to perform real-time inference on low-cost embedded platforms, in order to control a real-world robot platform.
% \newline An incremental approach will be used for the design and development of the proposed system. Thus, a single-task setting will first be evaluated, but, with respect to \cite{mandi2022towards_more_generalizable_one_shot}, it will be characterized by a high degree of variability based on different initial conditions and different objects to be manipulated so as to evaluate the ability to generalize across a single task, a setting in which meta-learning-based procedures have proven successful \cite{finn2017one_shot_visual_il,yu2018daml}. Next, the system will be extended into a multi-task context, where the hypotheses made earlier about using Variational-Autoencoder for learning a representation related to the demonstrated sub-tasks can be evaluated and validated in combinations with the use of Contrastive Loss, which have been shown to be valid in methods such as \cite{sermanet2018time_contrastive,zakka2022xirl} where the goal has been to learn a task representation from different embodiments \cite{zakka2022xirl} and different viewpoints \cite{sermanet2018time_contrastive}. Here, with respect to \cite{Mandlekar2020GTI}, the Variational Autoencoder will be combined with Contrastive Loss, to learn a sub-task related representation starting from demonstrations composed of different visual appearance. While, with respect to \cite{mandi2022towards_more_generalizable_one_shot}, the proposed combination aims to model explicitly the concept of sub-tasks, which may help the generalization in a multi-task settings by leveraging the task hierarchical structure.
