\section{Proposed Research Activity}
\label{sec:research_activity}
After the presentation of the context of interest and the aspects to be treated in the current research proposal, this section will formalize the improvement hypotheses reported in Section \ref{sec:research_topic}, identifying the main methods of interest from which will start the application of the proposed changes, and the procedures that will be used to validate and compare the proposed approach against the state-of-the-art methods.
Starting from the discussion made in Section \ref{sec:research_topic}, one question arises \textit{`'}

% \newline As discussed in Section \ref{sec:research_topic}, it was seen that current methods start from a representation of the environment that lacks geometric information and that is obtained from the point of view different from that of the robot, and in particular from that of the end-effector. 
% Concerning this point, starting from the state-of-the-art architecture \cite{mandi2022towards_more_generalizable_one_shot} based on Transformer, the goal will be to extend it in such a way that it will be able to handle time sequences characterized by RGB-D observations, and taken from a camera mounted on the end-effector. Several approaches can be used to achieve this. In fact, in the Computer Vision community, RGB-D images can be handled in different ways. For example, one can apply convolutional layers separately on the RGB and depth images, as in \cite{zhang2018deep_vr_teleoperation}, and subsequently perform a fusion between the extracted features based, for example, on their concatenation. Another approach may be using a 3D Point-Cloud Neural Network, such as \cite{qi2017pointnet++} used in \cite{fang2020graspnet}. To answer the question which of the two approaches to prefer, given that the reference method \cite{mandi2022towards_more_generalizable_one_shot} has a transformer-based architecture, a first step may be to apply convolutional filters separately on the RGB and D images and then combine them appropriately, going on to evaluate different fusion techniques, starting from simple concatenation to the use of a weighted sum, with the ultimate goal of generating the embedding as input to the transformer.
% \newline This setting will allow evaluating the hypothesis that adding geometric information can reduce failures caused by collisions with objects of interest or by the inability to determine when to close or open the gripper.  
% The resulting method will then be tested and compared first in simulation, taking advantage of the open-source simulation environments \cite{brockman2016openai,zhu2020robosuite}. Then, based on the obtained results, the proposed architecture will be tested and evaluated on a real robot platform, adding an additional experimental contribution that current methods such as \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot} lack.
% \newline As for the tasks of interest, the main focus will be devoted to industrial tasks (e.g., pick-and-place, push, peg-insertion), introducing an additional constraint with respect to \cite{mandi2022towards_more_generalizable_one_shot}, since the system will have to be able to perform real-time inference on low-cost embedded platforms, in order to control a real-world robot platform.
% \newline An incremental approach will be used for the design and development of the proposed system. Thus, a single-task setting will first be evaluated, but, with respect to \cite{mandi2022towards_more_generalizable_one_shot}, it will be characterized by a high degree of variability based on different initial conditions and different objects to be manipulated so as to evaluate the ability to generalize across a single task, a setting in which meta-learning-based procedures have proven successful \cite{finn2017one_shot_visual_il,yu2018daml}. Next, the system will be extended into a multi-task context, where the hypotheses made earlier about using Variational-Autoencoder for learning a representation related to the demonstrated sub-tasks can be evaluated and validated in combinations with the use of Contrastive Loss, which have been shown to be valid in methods such as \cite{sermanet2018time_contrastive,zakka2022xirl} where the goal has been to learn a task representation from different embodiments \cite{zakka2022xirl} and different viewpoints \cite{sermanet2018time_contrastive}. Here, with respect to \cite{Mandlekar2020GTI}, the Variational Autoencoder will be combined with Contrastive Loss, to learn a sub-task related representation starting from demonstrations composed of different visual appearance. While, with respect to \cite{mandi2022towards_more_generalizable_one_shot}, the proposed combination aims to model explicitly the concept of sub-tasks, which may help the generalization in a multi-task settings by leveraging the task hierarchical structure.
