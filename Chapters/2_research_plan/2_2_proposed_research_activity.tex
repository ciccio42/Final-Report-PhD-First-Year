\section{Proposed Research Activity}
\label{sec:research_activity}
In this section the proposed research activity will be presented, and linked with the gaps discussed in Section \ref{sec:research_gaps}. 
\newline As seen, the problem of Learning from Demonstration is far from being solved. Indeed, there is a relevant amount of open questions that cover both well-studied Behavioral Cloning and Inverse Reinforcement Learning methods and the novel Generative Adversarial Imitation Learning and Learning from Observation methods. 
From the study of the state of the art and the reported research gaps, it could be seen that, in recent years, there has been a particular interest on the two aspects,  
the first is related to the question \textit{``How can a system generalize both to different instances of the same task and to different tasks?"} and the second is related to the question \textit{``How can a human describe the task to be executed?"}. Developing and proposing a solution that can answer these two questions would result in a \textbf{highly adaptable system}. In fact, solving the first problem would result in a system that can be applied to different scenarios as it is, minimizing the effort required to fine-tune for the specific application. While solving the second problem through human video demonstrations and natural language descriptions would result in a highly human-centered human-machine interaction, thus making available a robot programming technique that follows a very natural and intuitive process. By keeping in mind these aspects, the final goal of this research activity is to investigate the possibility to design and develop an innovative Artificial Intelligence based system able to replicate a set of tasks given in input expert demonstrations represented by robot/human video performing a task, and further natural language task description. In particular, starting from the methods that have been presented in the state of the art, the aim is to make an improvement in both methodological and technological terms, going on to investigate the following aspects:

\paragraph{Dataset and Task representation} \mbox{} \\
As already discussed in Section \ref{sec:research_gaps} the dataset is one of the key element for an AI based system. With respect to the datasets collected by the presented methods, the goal will be to investigate the following:
    \begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item Whether the use of additional information, such as the frame label of the (sub-)task, can improve the ability of the method to extrapolate embeddings relevant to the (sub)task and infer the action based on the current observation of the robot (first-person or third-person visual information), the current state of the robot (joint position, speed), the label of the current (sub)task. 
    \item Closely related to (1), the second investigation related to the dataset and task representation concerns how the intent of the task can be provided most naturally and intuitively possible. Here, one aspect to be investigated is how a natural language description combined with a video demonstration can be exploited to act as conditioning or (self-)supervision during the training in order to extract both high-level task information (e.g., pick-and-place) and task-specific information (e.g., pick the green ball and place in the red cup).
    \end{enumerate*} Concerning these two points, the investigation will begin by collecting this information in a simulated environment, (e.g., robosuite \cite{zhu2020robosuite} or openai-gym \cite{brockman2016openai}). Since, it is easier and faster to automatically collect a massive dataset containing different scenarios and robotic platforms. Once extensive tests have been performed in simulation, and the pros and cons have been weighed, the next step will be designing and developing a real-world activity demonstration recording system for both human and robotic demonstration videos.  
\paragraph{Methods and learning procedure} \mbox{} \\
The investigation of methods and learning procedures is strictly related to the dataset and task representation. As already discussed, the goal is to obtain a vision-based multi-task imitation learning system. As seen in the State-of-the-Art section, learning-based approaches are essential for solving this problem. Building upon the currently proposed methods, the investigation will be devoted to designing and developing a system based on modern architectures (e.g., Recurrent Neural Network, Transformers, (conditional) Variational-Autoencoder) that have been proved to be effective in learning contextual information, and latent-space representation from temporal sequences. The challenge here is to apply such methods in multi-task learning from demonstration. In this setting, both high-level and task-specif embeddings must be extracted and combined in input video and natural language task descriptions. The experimental validation will be performed first in simulation, where pros and cons could be identified in a safe environment. Then based on the obtained results, the system will be tested on a real-world robot platform.
\paragraph{Computational efficiency and real-time constraint} \mbox{} \\
Deep Learning architectures are known to be able to solve very complex problems. Still, they generally require an appropriate amount of computational resources and capability to produce an output in a reasonable time. This problem becomes critical when the system must control an agent interacting with the environment by performing actions. In proposing a novel method, a particular focus must be posed on the computational requirements of the method itself. The right trade-off between two properties must be found: (1) the inference time, which is in general as low as the complexity of the system decreases; (2) the system accuracy, which is as high as the complexity of the system increases. Generally speaking, two possible solutions can be found for real-world robot applications. The first is to deploy the system on an expensive workstation connected to the robot through some communication protocol. The second is related to cloud-based solutions, where the method is deployed on an external server, communicating through Internet connection. Generally speaking, since these proposed systems must work on light-load cobots or social robots, a workstation-based solution can be inadequate regarding the total cost. In contrast, a cloud-based solution can be inadequate because the internet connection may be missing or introduce unacceptable delays. A third, less explored solution is based on running the system on a low-cost embedded platform (e.g., Nvidia Jetson Xavier) connected directly to the robot. Based on all these considerations, the final goal is to design and deploy a system that can solve the problem at hand with a reasonable success-rate while running on an embedded platform.
