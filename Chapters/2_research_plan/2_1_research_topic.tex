\section{Research Gaps}
\label{sec:research_gaps}
Regardless of the application area, whether industrial or social, one of the main goals of robotics is to have a system capable of performing various tasks in dynamic environments characterized by a high degree of uncertainty. This system must be characterized by several properties, among which are \textbf{high adaptability} to new scenarios and operating conditions, \textbf{high reliability} in execution, and it must be able to run in \textbf{real-time} on \textbf{real-world robot systems}. As seen from the study of the literature, learning-based methods appear to be of interest, given their ability to infer a task execution policy from a series of demonstrations, mainly when the application context is characterized by high uncertainty, such as the location or type of the target object or the task itself that has to be executed. Although the problem of Learning from Demonstration has been studied for many decades, there are still substantial gaps at different levels of abstraction. Gaps can be identified that are both methodological and purely applicative.

Regarding the methodological aspects, there are different questions that, despite being studied, have not yet been fully answered, these gaps will be briefly discussed.
\newline \textbf{\textit{How can the compounding-error be mitigated?}} As reported in Section \ref{sec:sota}, the compounding-error is the most important limitation related to Imitation Learning. Solutions proposed in the literature are mainly based on algorithms that require active interaction between the learner and the expert. This may lead to both safety problems and time-consuming learning procedures, especially for complex long-horizons tasks. While in the context of autonomous vehicle driving is prohibitive and very risky to generate bad examples, in the context of robotic manipulation, it may have sense to produce a set of low-quality demonstrations (e.g., demonstrations where the robot fails to pick the target object), label these demonstrations with a quality index (e.g., based on execution time, smoothness of the performed action) or event classes (e.g., ``approaching" class for those frames where the robots approaches the target object, ``closing gripper" for those frames where the robot closes the gripper, etc\dots), then show recovery behaviors and label them. In this way, the system can learn the correct task execution and actions to be performed in case of failure. In this case the main challenge is to design an architecture able to identify a divergence situation, and performing recovery actions, based on mainly exteroceptive sensors. 
\newline \textbf{\textit{How can a system generalize both to different instances of the same task and to different tasks?}} When it comes to systems based on Artificial Intelligence techniques, the leading property of this should be the generalization capability. In applying such systems in the context of task control and execution, there are at least two levels of generalization. The first is a single-task generalization capability, and the second is a multi-task generalization capability. The former is related to the desire to obtain a system that can correctly execute different instances of the same task (e.g., different objects and/or different initial conditions). The latter, on the other hand, is related to the desire to have a system able to solve different tasks without complete retraining, which can be time-consuming and data inefficient. To achieve this goal, recent methods proposed to use Meta-Learning algorithms, showing promising results but with relatively low overall performance. Improvement can be achieved by better exploitation of temporal information and by the improvement of learning procedures and architectures that must be guided by the goal of extrapolating (sub-)task related embeddings, independent of context (e.g., demonstrator and workspace visual appearances) so that they can be reused and combined for the execution of new instances or new tasks. Moreover, given the relatively low ammount of data in the produced dataset, the exploitation of temporal information should be combined with efficient and meaningful task representation, given by latent-variable in Model-Based methods.
\newline \textbf{\textit{How can a dataset be reused across different works?}} When AI-based methods are used, the dataset plays a key role. The ability to train and test methods on a common dataset is crucial for a fair comparison, for pointing out methods' criticality, and consequently, for findings hints for possible improvements. Generally speaking, dataset collection is a tedious and time-consuming process, and this is particularly true when collecting task demonstrations. Having the ability to reuse a dataset can save time, allowing a greater focus on the learning procedure and overall architecture, increasing potentially the rate of advancements. Unfortunately different aspects may severely limit the reuse of a dataset, for example, the used robotic platform or the different visual appearance. The former plays a critical role, especially when robot-specific information, such as the joints' position/velocity/torque, are used. %An interesting aspect may be to develop a system that can generalize to different robotic platforms.
In order to avoid the naive approach based on collecting demonstrations by using different robots, a starting point could be the collection of a dataset that contains platform-independent information, such as the trajectory of the manipulated object or video of a human performing the desired task. This solution leads to the other important challenge, that is the difference in appearance between a public dataset and the actual testing conditions (e.g., different objects but of the same category, different backgrounds, viewpoints etc.). To solve these problems, one might think of using simulated environments, which allows to train and test different methods on the same environment. However, in the end, the system must be deployed in a real-world context, and the problem of Sim-to-Real Transfer must be handled. Instead of having a shared environment to use for training and testing the proposed methods, an alternative solution can be found by formalizing the problem differently. In fact, from datasets showing different demonstrators (both human and robot) in different environments and with different objects but performing the same task, the goal can be to develop a training procedure and architecture capable of extrapolating task-dependent embeddings but context agnostic. This setting is very similar to Learning from Observation, where early examples of applications of these concepts can be found \cite{sermanet2018time_contrastive,zakka2022xirl}.
\newline \textbf{\textit{How can the perception errors be mitigated?}} As stated in the paragraph dedicated to Behavioral Cloning (pag. \pageref{sec:lfo}), one of the main reasons for task failure is related to errors in perception and identification of key points during task execution (e.g., failure to close the gripper on object approach, hitting the object of interest because the system does not recognize its presence). A possible way to solve these problems is to use a multi-modal perception system, which could include tactile and/or proximity information in addition to RGB(-D) visual information, in order to explicitly model, in the demonstrations, the fact that the robot touches an object then it closes the gripper. Moreover, the idea behind Hierarchical Task Decomposition methods may also play an important role in solving such problems since they model sub-policy activation/deactivation when a given event is detected, however the event detection should not based on hand-written conditions (e.g., distance between objects computed through the identification of specific markers), but the system should be able to learn, and then produce automatically the sub-tasks relationships. With respect to current solutions, the main challenge is to guarantee a certain level of explainability and safety of the performed actions, in order to increase the reliability of such systems, and move towards systems that can actively collaborate with human to perform a task.
\newline \textbf{\textit{How can a human describe the task to be executed?}} Being able to express the intent as efficiently and naturally as possible can lead to several benefits, both in terms of pure human-robot interaction (e.g., during the system operation, the human can describe the task to execute using a voice command), and during the training procedure itself. Regarding this last point, the human intent can be described by means of video demonstrations and natural language task descriptions. These kinds of intent representation can be exploited: \begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item in the multi-task setting, where they can be used as policy conditioning, in this way, the policy can infer the action with respect to both the current observation and the required task;
    \item in the LfO approach, where the whole dataset can be composed of only unlabeled video of human performing a task.
\end{enumerate*} 
It is reasonable to think that having either a system able to infer task-relevant features as effectively as possible may increase the training efficiency and the overall system performance. The former is a very important aspect to consider because in the context of robotics it is not so trivial to obtain the amount of samples that are usually needed to train a system based on deep architectures.

As for the questions more related to application aspects. Based on the proposed state of the art, there is one that tower above all others, mainly related to the approaches of IRL, GAIL, and LfO, that will be discussed below.
\newline \textbf{\textit{Can these approaches be effectively used for solving complex manipulation tasks on real robot platforms?}} In particular, the theoretical properties behind training frameworks related to IRL and GAIL are well known. In addition, proposed experiments both in simulation and real environments on more or less complex tasks have shown promising results, especially for methods based on GAIL and LfO. However, on the basis of current knowledge, there is a lack of experimental analyses in real environments that can point out flaws of such approaches when applied in a real context and thus guide the development of new methods to solve them. For example, most LfO methods applied in robot manipulation are based on some image-to-image translation, where the video of a human demonstration is translated into the corresponding robot video by means of complex generative architecture, but, instead of using the video demonstration as it is, \textit{can a more efficient and effective task representation be obtained?} Such representation could be an object centric trajectory extrapolated from the human demonstration, or a set of keypoints tracked from the human arm movements. In this way, the problem becomes either to directly find the robot action that generates the desired object movement, or find the mapping between human and robot motions, in order to solve the task.

All the gaps presented up to now are stricly related each others, and focusing on a specific aspect can lead to hints for solving other gaps.