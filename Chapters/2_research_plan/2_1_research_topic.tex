\section{Research Topic}
\label{sec:research_topic}
In general, this research activity aims to address the problem of Learning from Demonstration, where the goal is to obtain a policy, i.e., a function that maps a sequence of states into the corresponding sequence of actions, from a set of examples. Specifically, with respect to the general problem and the requirements given in Section \ref{sec:sota}, especially the requirement of high-adaptability, the goal is to design a \textbf{Vision-based Imitation Learning} system capable of learning the execution of \textbf{different tasks},  starting from a set of demonstrations. The resulting policy must be able to infer the current action from a visual observation of the environment from the robot's point of view. Such a system can find applications both in industrial (e.g., ask to a robot to pick a tool and then use it) and social robotic scenarios (e.g., ask to a housecare robot to move an object from one point to another), altough the main focus will be posed on Industrial tasks (e.g., pick-and-place, pushing, peg-insertion). 
Among the four approaches presented in Section \ref{sec:sota} the one that is most suitable for the context of interest is \textit{Behavioral Cloning}, in particular those methods that leverage \textit{Meta-Learning} algorithms \cite{finn2017one_shot_visual_il,yu2018one_shot_hil,yu2018daml} or in general train a \textit{Multi-Task Imitation Learning} system \cite{jang2022bc_z,mandi2022towards_more_generalizable_one_shot}. From Table \ref{table:mosaic} it can be noted that in terms of pure success-rate both in the Single-Task and in the Multi-Task Setting the problem is far from being solved, and going from the first setting to the second one is no trivial. Indeed, also in the most ``simple" setting, i.e., when instances of the tested task are used during the training, the success rate decreases for almost all the tasks. These results points out the potential of the proposed multi-task imitation learning systems, since, as reported in the works \cite{jang2022bc_z,yu2018daml}, the main reason for the failure of these systems lies in errors in the identification of the critical points of a task and the subsequent execution of the correct action, instead of in the general identification of the intent of the task to be performed, represented for example by the identification of the target object.
Starting from this general consideration regarding the state of the art, the current research proposal aims to focus on two closely related aspects, which are described in the paragraph in the following paragraphs.
\paragraph{\textit{What is the best representation of the task to facilitate the identification of critical points?}}  \mbox{} \\
In the works currently proposed in the literature, the dataset is mainly composed of a combination of demonstration videos showing the robot task execution and the action performed by the robot itself, represented by the linear and angular velocity of the end-effector \cite{yu2018daml} or the desired pose of the end-effector \cite{dasari2021transformers_one_shot,jang2022bc_z}. Regarding the ability to identify critical points within the task, the criticality of the proposed solutions lies in the fact that the visual observations are composed of RGB images and recorded with respect to a third-person viewpoint \cite{yu2018daml,jang2022bc_z}. With this setting, at least two important critical issues are generated: The first is related to the lack of geometric information, e.g., the relative distance between the gripper and the object being approached. The second is related to the fact that the observations represent a different point of view from that of the robot, which can lead to the creation of occlusions generated by the movement of the arm itself, further complicating the detection of critical points such as the fact that the object is close and therefore the gripper can be closed. With respect to these problems, a possible solution could be to augment the observations by also adding a fourth component represented by the depth image, and also recording the movements from a robot's point of view, for example, by mounting a camera attached the gripper, as is usually done in Vision-Based grasping systems \cite{fang2020graspnet}.
\newline Moreover, another important aspect related to the task representation is the lack of semantic information that can lead to a correct and meaningful task segmentation.
It would be fascinating to obtain a system capable of automatically extracting subtasks from a set of demonstrations of different tasks composed of the same primitives, e.g., approach and push. Current methods \cite{jang2022bc_z,mandi2022towards_more_generalizable_one_shot} have demonstrated a limitation in solving this problem, evidenced especially by the performance decay in the multi-task setting compared to the single-task setting \cite{mandi2022towards_more_generalizable_one_shot,zhang2018deep_vr_teleoperation}, highlighting how training a monolithic architecture that does not explicitly consider task segmentation can lead to performance that is too low for real-world application. Work that has attempted to handle automatic task segmentation has been \cite{yu2018one_shot_hil,Mandlekar2020GTI}. Specifically, in \cite{yu2018one_shot_hil}, a task-phase predictor was trained on a set of primitive demonstrations (e.g., pushing, lifting, placing) based on their length to be used later as a sub-task selector. In \cite{Mandlekar2020GTI}, a goal-conditioned Variational-Autoencoder was trained to identify an efficient representation of the sub-task to be executed, to be used later for training a conditional policy on that representation. With respect to these methods, and taking inspiration from action-recognition-based methods \cite{goyal2017something2something}, an alternative approach may be based on using sub-task action labels, then training a multi-task architecture capable of generating in output both the action for task execution and a label related to the running sub-task that the robot is executing.

\paragraph{\textit{What is the best architecture for modeling a policy?}}  \mbox{} \\
A task demonstration extends over two dimensions, the first is the temporal one, and the second is the spatial one. Different types of architectures have been used in the context of Learning from Demonstration, starting from the classic Convolutional Neural Network \cite{zhang2018deep_vr_teleoperation,yu2018daml,yu2018one_shot_hil} up to the modern architectures based on Transformers \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}. From different results reported in \cite{mandi2022towards_more_generalizable_one_shot,mandlekar2022matters}, it was possible to observe how the use of architectures capable of managing time sequences is crucial to obtain a system with a high success rate. In the context of multi-task learning, however, an additional level must be added, linked to the representation of the tasks and their respective sub-tasks. It is reasonable to think that having an architecture capable of obtaining a representation linked to the sub-tasks performed in the different tasks may lead to greater generalization across the tasks. The architecture closest to this modeling is \cite{Mandlekar2020GTI}, which uses a conditional Variational Autoencoder to learn a representation of the sub-task starting from the current observation and the observation representative of the final state. In this paper, however, the visual appearance of the proposed tasks was the same, an assumption that may be too strong in a more general context, such as that proposed by \cite{mandi2022towards_more_generalizable_one_shot}. A possible improvement concerning this setting can be represented by the use of Contrastive Loss, used in \cite{sermanet2018time_contrastive,zakka2022xirl}, in order to support the generation of a representation as similar as possible for the same sub-tasks but coming from different contexts and as different as possible for different sub-tasks