\section{Research Topic}
\label{sec:research_topic}
One of the primary objectives of robotics is to develop autonomous robots capable of performing manipulation tasks, such as pick-and-place or assembly operations. These tasks are prevalent in both industrial and social robotics domains. While traditional industrial operations can be addressed with hand-written controllers based on known objects and fixed actions, the current landscape of social robotics and modern industrial applications demands a greater level of adaptability and flexibility. These scenarios require robots to handle dynamic environments, recognize diverse objects, estimate their positions, and adapt their actions accordingly.
\newline In this scenario, traditional hand-written controllers are not well-suited, and there has been a notable shift towards data-driven approaches based on Machine Learning and Deep Learning algorithms over the past decade.
\newline Instead of relying on explicitly programmed rules and control algorithms, data-driven approaches leverage large amounts of data to enable robots to learn from examples and make intelligent decisions. Machine Learning techniques, such as supervised learning, unsupervised learning, and reinforcement learning, have become increasingly popular in robotics.
\newline In this context, this research activity aims to address the problem of Learning from Demonstration, where the goal is to obtain a policy, i.e., a function that maps a sequence of states into the corresponding sequence of actions, from a set of examples. Specifically, with respect to the general problem and the requirements given in Section \ref{sec:sota}, especially the requirement of high-adaptability, the goal is to design a \textbf{Vision-based Imitation Learning} system capable of learning the execution of \textbf{different tasks},  starting from a set of demonstrations. The resulting policy must be able to infer the current action from a visual observation of the environment from the robot's point of view. Such a system can find applications both in industrial (e.g., ask to a robot to pick a tool and then use it) and social robotic scenarios (e.g., ask to a housecare robot to move an object from one point to another), altough the main focus will be posed on Industrial tasks (e.g., pick-and-place, pushing, peg-insertion).
Among the four approaches presented in Section \ref{sec:sota} the one that is most suitable for the context of interest is \textit{Behavioral Cloning}, in particular those methods that leverage \textit{Meta-Learning} algorithms \cite{finn2017one_shot_visual_il,yu2018one_shot_hil,yu2018daml} or in general train a \textit{Multi-Task Imitation Learning} system \cite{jang2022bc_z,mandi2022towards_more_generalizable_one_shot}. From Table \ref{table:mosaic} it can be noted that in terms of pure success-rate both in the Single-Task and in the Multi-Task Setting the problem is far from being solved, and going from the first setting to the second one is no trivial. Indeed, also in the most ``simple" setting, i.e., when instances of the tested task are used during the training, the success rate decreases for almost all the tasks. These results points out the potential of the proposed multi-task imitation learning systems, since, as reported in the works \cite{jang2022bc_z,yu2018daml}, the main reason for the failure of these systems lies in errors in the identification of the critical points of a task and the subsequent execution of the correct action, instead of in the general identification of the intent of the task to be performed.
In addition to the overall assessment we conducted following the literature review, it is essential to emphasize that significant modifications have been made in relation to the Research Proposal presented at the conclusion of the first year. These alterations were prompted by interesting insights obtained from preliminary experiments, which subsequently enabled us to identify a previously unexplored research gap and formulate the ensuing research question:
\paragraph{\textit{Reason-then-act: How the separation between perception and action can affect the performance of an Imitation-Learning System}} \mbox{} \\
In both \textit{``Language-Conditioned Imitation Learning"} and \textit{``Video-Conditioned Imitation Learning"}, the primary objective is to obtain a function $\pi: S \times C \rightarrow A $, that can effectively map the current state and command pair to the corresponding action. These systems need to address two key challenges:
\begin{enumerate}
    \item \textbf{Generate Meaningful Semantic Information}: The first challenge involves the generation of meaningful semantic information from the command. This requires understanding the command's content, identifying relevant objects or actions mentioned, and encoding this information in a way that is useful for the task. In language-conditioned systems, natural language processing techniques, such as text parsing, semantic analysis, or neural language models \cite{stepputtis2020language,jang2022bc_z,brohan2022rt}, are commonly employed to extract and represent the semantics of the command. In image-conditioned systems, image processing and computer vision techniques (e.g., deep learning architectures \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}) are used to extract relevant features from the image query.
    \item \textbf{Fuse Information from Current State and Command}: The second challenge is to fuse the information extracted from the current observed state and the command in a meaningful way. This fusion aims to create an intermediate embedding or representation that captures the relevant aspects from both inputs. Techniques such as feature concatenation \cite{james2018task_embedded,stepputtis2020language,bhutani2022attentive_one_shot}, attention mechanisms \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}, or feature-wise linear modulation \cite{brohan2022rt} are utilized to combine the visual information from the state and the semantic information from the command. The fused representation should capture the important cues for inferring the correct action based on the current input and command.
\end{enumerate}
Despite the progress made with Multi-Task Imitation Learning, the performance of these agents still lacks consistent robustness when faced with challenges such as distractor objects and varying backgrounds \cite{brohan2022rt}. During our preliminary experimentation we tried to emphasize the first problem in order to \textbf{focus our attention to scene analysis and cognitive aspects} rather than pure control problems, by testing the models in scenarios where there are objects that difference for very high-level features such as the color. Indeed, our preliminary experiments revealed a noteworthy observation. While our system consistently generates what can be referred to as ``valid trajectories", i.e, it effectively carries out the actions of picking up an object and correctly placing it in the intended position, it often struggles with the critical task of identifying the precise target object. In other words, there is a recurrent issue where the selected object for manipulation is not the correct one.
\newline The observed results can be explained through a comprehensive analysis of the underlying learning procedure. Specifically, the reference methods \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt} are trained using the principles of Behavioral Cloning (BC). BC methodologies primarily concentrate on determining the appropriate action to execute within a given context. This approach has exhibited effectiveness in single-task Imitation Learning scenarios \cite{zhang2018deep_vr_teleoperation,duan2017one_shot_il} and in multi-task settings where scenes are comprised of easily distinguishable objects, as demonstrated in \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt}. Nevertheless, it is crucial to highlight that this framework operates under the presumption that there is an inherent solution to the initial challenges associated with comprehending the tasks in question. In simpler terms, it assumes that the system can seamlessly progress from high-level inputs, including images and textual information, toward the creation of a meaningful task representation that informs the process of action inference. While this methodology remains viable in straightforward scenarios, it may prove insufficient in intricate situations where objects exhibit significant variations in high-level characteristics. In such complex scenarios, the cognitive task becomes considerably more intricate and may be beyond resolution solely through the information derived from the action-based loss. In complex scenarios where high-level object features exhibit substantial variability, the conventional approach of assuming a seamless transition from input data to meaningful task representation may be inadequate. Such intricate situations require more advanced strategies to address the complexities and variations in the tasks effectively.
Starting from all the consideration above, the stated problem can be faced in two ways:
\begin{enumerate}
    \item Enhancing dataset diversity by expanding the representation of objects and employing a large-scale model capable of assimilating all the knowledge inherent in the dataset.
    \item Dividing the Decision-Making Problem into two primary blocks, namely a reasoning module and an action module. This segmentation enables the independent validation of each task within the decision-making process and the separate evaluation of the performance of each block. Subsequently, a method should be devised to integrate these two components, creating a modular system capable of solving complex problems. This system would consist of interconnected modules designed to address simpler problems.
\end{enumerate}
In our second year of research, we opted to concentrate on a procedure aligned with the second approach. This choice was motivated by its effectiveness in assessing the various sub-tasks inherent in a decision-making problem, such as the control of a robotic system. Comprehensive details regarding the exact procedure and its formalization will be presented in the forthcoming Section \ref{sec:research_activity}.
