\section{Research Gaps}
\label{sec:research_gaps}
Regardless of the application area, whether industrial or social, one of the main goals of robotics is to have a system capable of performing various tasks in dynamic environments characterized by a high degree of uncertainty. This system must be characterized by several properties, among which are high reliability in execution, high adaptability to new scenarios and operating conditions, and high simplicity in robot programming. As seen from the study of the literature, learning-based methods appear to be of interest, given their ability to infer a task execution policy from a series of demonstrations, mainly when the application context is characterized by high uncertainty, such as the location or type of the target object or the task itself that has to be executed. Although the problem of Learning from Demonstration has been studied for many decades, there are still substantial gaps at different levels of abstraction. Gaps can be identified that are both methodological and purely applicative.

Regarding the methodological aspects, there are different questions that, despite being studied, have not yet been fully answered, these gaps will be briefly discussed.
\newline \textbf{\textit{How can the compounding-error be mitigated?}} As reported in Section \ref{sec:sota}, the compounding-error is the most important limitation related to Imitation Learning. Solutions proposed in the literature are mainly based on algorithms that require active interaction between the learner and the demonstrator. This may lead to both safety, and data-efficiency problems for complex long-horizons tasks. While in the context of autonomous vehicle driving is prohibitive and very risky to generate bad examples, in the context of robotic manipulation, it may have sense to produce a set of wrong demonstrations (e.g., demonstrations where the robot fails to pick the target object), quantitatively evaluate the quality of a demonstration (e.g., based on execution time, or smoothness of performed actions), and show recovery behaviors. In this way, the system can learn the correct task execution and actions to be performed in case of failure. In this case the main challenge is to design an architecture able to identify a divergence situation, and performing recovery actions, based on mainly exteroceptive sensors. 
\newline \textbf{\textit{How can a system generalize both to different instances of the same task and to different tasks?}} When it comes to systems based on Artificial Intelligence techniques, the leading property of this should be the generalization capability. In applying such systems in the context of task control and execution, there are at least two levels of generalization. The first is a single-task generalization capability, and the second is a multi-task generalization capability. The former is related to the desire to obtain a system that can correctly execute different instances of the same task (e.g., different objects and/or different initial conditions). The latter, on the other hand, is related to the desire to have a system able to solve different tasks without complete retraining, which can be time-consuming and data inefficient. To achieve this goal, recent methods proposed to use Meta-Learning algorithms, showing promising results but with relatively low overall performance. A possible trace for improvements can be represented by a better exploitation of temporal information. In particular, given the relatively low ammount of data in the produced dataset, the exploitation of temporal information should be combined with efficient and meaningful task representation, given by latent-variable in Model-Based methods.
\newline \textbf{\textit{How can a dataset be reused across different works?}} Different aspects may severely limit the reuse of a dataset used in a given work, for example, the robotic platform used or the different visual appearance. The former plays a critical role, especially when robot-specif information are used, such as the joint's position/velocity/torque generated task execution. An interesting aspect may be to develop a system that can generalize to different robotic platforms and a starting point could be the collection of a dataset which contains information independent of the robotic platform, such as, the trajectory of the manipulated object or video of human performing the desired task. The other important challenge is the difference in appearance between a public dataset and the actual testing conditions (e.g., different objects but of the same category, different backgrounds, viewpoints etc.). To solve this problem, one might think of using simulated environments, in which case one must address the problem of Sim-to-Real Transfer, caused precisely by the difference in visual appearance between the simulated and real worlds. 
An alternative solution could be to train a system in such a way that the generated visual embeddings are as general as possible (i.e., an embedding that encodes spatial-temporal information related to the category of the manipulated object rather than the specific object information).
\newline \textbf{\textit{How can the perception errors be mitigated?}} As stated in the paragraph dedicated to Behavioral Cloning (pag. \pageref{sec:lfo}), one of the main reasons for task failure is related to errors in perception and identification of key points during task execution (e.g., failure to close the gripper on object approach, hitting the object of interest because the system does not recognize its presence). A possible way to solve these problems is to use a multi-modal perception system, which could include tactile and/or proximity information in addition to RGB(-D) visual information, in order to explicitly model, in the demonstrations, the fact that the robot touches an object then it closes the gripper. Moreover, the idea behind Hierarchical Task Decomposition methods may also play an important role in solving such problems since they model sub-policy activation/deactivation when a given event is detected, however the event detection should not based on hand-written conditions (e.g., distance between objects computed through the identification of specific markers), but the system should be able to learn, and then produce automatically the sub-tasks relationships. With respect to current solutions, the main challenge is to guarantee a certain level of explainability and safety of the performed actions, in order to increase the reliability of such systems, and move towards systems that can actively collaborate with human to perform a task.
\newline \textbf{\textit{How can a human describe the task to be executed?}} Being able to express the intent as efficiently and naturally as possible can lead to several benefits, both in terms of pure human-robot interaction (e.g., during the system operation, the human can describe the task to execute using a voice command), and during the training procedure itself. Regarding this last point, the human intent can be described by means of video demonstrations or voice command. These can be useful for two reasons, \begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item in a multi-task setting, natural language description and/or human video can be used as policy conditioning;
    \item in the LfO approach, the whole dataset is composed of only unlabeled video of human performing a task.
\end{enumerate*} It is reasonable to think that having either a system able to infer task-relevant features as effectively as possible or a task characterization as task-oriented as possible may increase the training efficiency and the overall system performance. The latter is a very important aspect to consider because in the context of robotics it is not so trivial to obtain the amount of samples that are usually needed to train a system based on deep architectures.

As for the questions more related to application aspects. Based on the proposed state of the art, there is one that tower above all others, mainly related to the approaches of IRL, GAIL, and LfO, that will be discussed below.
\newline \textbf{\textit{Can these approaches be effectively used for solving complex manipulation tasks on real robotic platforms?}} In particular, the theoretical properties behind training frameworks related to IRL and GAIL are well known. In addition, proposed experiments both in simulation and real environments on more or less complex tasks have shown promising results, especially for methods based on GAIL and LfO. However, on the basis of current knowledge, there is a lack of experimental analyses in real environments that can point out flaws of such approaches when applied in a real context and thus guide the development of new methods to solve them. For example, most LfO methods applied in robot manipulation are based on some image-to-image translation, where the video of a human demonstration is translated into the corresponding robot video by means of complex generative architecture, but, instead of using the video demonstration as it is, \textit{can a more efficient and effective task representation be obtained?} Such representation could be an object centric trajectory extrapolated from the human demonstration, or a set of keypoints tracked from the human arm movements. In this way, the problem becomes either to directly find the robot action that generates the desired object movement, or find the mapping between human and robot motions, in order to solve the task.

Obviously, all the gaps presented up to now are stricly related each others, and focusing on a specific aspect can lead to hints for solving other gaps.