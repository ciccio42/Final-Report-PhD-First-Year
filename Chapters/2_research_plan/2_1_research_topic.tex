\section{Research Topic}
\label{sec:research_topic}
% General descritpion
One of the primary objectives in robotics is to develop autonomous robots capable of performing a wide range of manipulation tasks, such as pick-and-place and assembly operations, in response to specific commands. These tasks are often characterized by some degrees of variability, which may be related to factors like object categories and positions.
\newline In the context of traditional industrial operations, manually coded control rules can effectively manage situations in which robots follow fixed and repetitive paths. This is achieved through prior knowledge of object categories and positions, within an environment where the robot's working area is known and fixed over time(e.g., the workcell in Figure \ref{fig:industrial_robots_example}).
In contrast to this scenario, the contemporary landscape of social robotics and modern industrial applications demands a higher level of adaptability and flexibility. Robots are expected to work in environments alongside human operators, receiving commands and engaging in interactions with them in a collaborative or cooperative manner. For example, a robot may be tasked with picking up a tool and delivering it to the operator. This entails the robots' ability to recognize various object categories and estimate their positions but also correlate the outcomes of environmental analysis with the requested commands and adapt their actions accordling, showing ``intellingent'' behaviors.

% Introduction to target methods
As extensively discussed in Chapter \ref{chapter:background}, the scientific community has directed its attention towards addressing these challenges by evaluating the utilization of \textit{data-driven} approaches. These methods encompass Imitation Learning algorithms, which utilize data derived from examples of desired behaviors, often referred to as demonstrations, to equip a robot with the capability to replicate the demonstrated tasks.
\newline Multi-Task Imitation Learning methods, as highlighted in \cite{jang2022bc_z, dasari2021transformers_one_shot, mandi2022towards_more_generalizable_one_shot, brohan2022rt}, are notably promising due to their capacity to fulfill the requirements of both high \textit{adaptability} and \textit{flexibility}. As extensively explained in Section \ref{sec:methods}, these methods use a \textbf{multi-task dataset} $\mathcal{D}^{E} = \left \{ \mathcal{D}_{1}, \dots \mathcal{D}_{n}\right \}$, which contains demonstrations for $n$ different tasks (e.g., $\mathcal{D}_{1}$ contains demonstrations of pick-place task, $\mathcal{D}_{2}$ contains demonstrations of nut-assembly task and so forth), to fit a \textit{single control function} denoted as $\pi^{L}_{\theta}(a_{t}|s_{t}, c_{m_{i}})$, that is able to map the current observed state $s_{t}$ and the command $c_{m_{i}}$, into the corresponding action $a_{t}$ that will be executed by a phisical robot. The command $c_{m_{i}}$ referes to the $m^{th}$ variation of the $i^{th}$ task (e.g., the pick-place task can have different variations based on the target object and the target placing spot as depicted in Figure \ref{fig:mosaic_tasks}). Tables \ref{table:mosaic} and \ref{table:rt1_results} illustrate the potential of the proposed approaches, yet challenges remain in both single-task and multi-task problems. As reported in \cite{jang2022bc_z, yu2018daml}, existing systems face difficulties in identifying critical task points, such as determining when to close the gripper near an object or open it in proximity to the placement area, rather than understanding the broader intent of the task at hand. Furthermore, performance drops occur when scenes involve distractor objects, i.e.,  objects  that  do  not  contribute  to  the  task  execution. This highlights the need for improved capabilities in correctly correlating commands with the results of scene analysis to identify target objects.

% Propose research activity
In the context presented so far, this research activity aims to address the problem of \textit{Learning from Demonstration} in multi-task scenarios. Our aim is to develop a system that can make a step towards the realization of a \textit{versatile collaborative robot} suitable for industrial applications. This robot should possess the capability to execute tasks directed by a human operator and acquire new skills based on a limited number of demonstrations, building upon its existing knowledge. To illustrate potential applications, consider a collaborative workspace where a human operator could instruct the robot to provide a tool or engage in assembly tasks.
\newline With respect to the general formulation extensively explained in Section \ref{sec:methods}, we are interested in exploring methods that rely on \textit{visual \ inputs}, i.e., the current state $s_{t}$ is an image that depicts the current state of robot workspace, and the command $c_{m_{i}}$ is given in terms of video demonstrations of an agent (e.g., robot or human) that executes the desired task.
\newline In addition to the overall assessment we conducted following the literature review, it is essential to emphasize that significant modifications have been made in relation to the Research Proposal presented at the conclusion of the first year. These alterations were prompted by interesting insights obtained from preliminary experiments, which subsequently enabled us to identify a previously unexplored research gap and formulate the ensuing research question:
\paragraph{\textit{Reason-then-act: How the separation between perception and action can affect the performance of an Imitation-Learning System}} \mbox{} \\
In both \textit{``Language-Conditioned Imitation Learning"} and \textit{``Video-Conditioned Imitation Learning"}, the primary objective is to obtain a function $\pi_{\theta}^{L}$, that can effectively map the current state and command pair to the corresponding action. These systems need to address two key challenges:
\begin{enumerate}
    \item \textbf{Command analysis and understanding}: The first challenge involves the
          analysis of the given command. Specifically, from an \textbf{high-level task command} the system should be able to understand the \textbf{intent} (e.g., picking and placing rather than assembling), identify the relevant objects (e.g., selecting the blue box rather than the red one), and recognize the mentioned actions (e.g., reach, follow, pick). In language-conditioned systems, neural language models \cite{stepputtis2020language,jang2022bc_z,brohan2022rt}, are commonly employed to extract and represent the semantics of the command. In image-conditioned systems, image processing and computer vision techniques (e.g., deep learning architectures \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}) are used to extract relevant features from the video demonstration.
    \item \textbf{Action Generation}: The second challenge is to correlate the features generated from the analysis of the observed state and the information obtained from the command, with the aim to create an intermediate representation that captures the relevant aspects from both inputs (e.g., there is a focus on the image portion that contains the target object). Techniques such as feature concatenation \cite{james2018task_embedded,stepputtis2020language,bhutani2022attentive_one_shot}, attention mechanisms \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}, or feature-wise linear modulation \cite{brohan2022rt} are utilized to combine the visual information from the state and the information from the command. The fused representation should capture the important cues for inferring the correct action based on the current input and command.
\end{enumerate}
Despite the progress made with Multi-Task Imitation Learning, the performance of these agents still lacks consistent robustness when faced with challenges such as distractor objects and varying backgrounds \cite{brohan2022rt}. During our preliminary experiments (Chapter \ref{chapter:preliminary_results}) we tried to emphasize this problem in order to \textbf{focus our attention to scene analysis and cognitive aspects} rather than pure control problems, by testing the models in scenarios where there are objects that differ in very high-level features such as the color. Indeed, our preliminary experiments revealed a noteworthy observation. While our system consistently generates what can be referred to as ``valid trajectories", i.e., it effectively carries out the actions of picking up an object and correctly placing it in the intended position, it often struggles with the critical task of identifying the precise target object. In other words, there is a recurrent issue where the selected object for manipulation is not the correct one.
\newline The observed results can be explained through a comprehensive analysis of the underlying learning procedure. Specifically, the reference methods \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt} propose end-to-end architectures that starting from \textbf{high-level inputs} (e.g., images and text) directly generate the required action, assuming that there is an implicit solution to the initial challenges associated with the command analysis and comprehension. These architectures are trained based on the principles of Behavioral Cloning (BC). BC offers two main approaches: one involves minimizing the disparity between the predicted and correct actions (as shown in Formula \ref{eq:mse}), while the other centers on maximizing the likelihood of executing the correct action (as shown in Formula \ref{eq:nll}). The choice between these approaches hinges on the nature of the policy, whether deterministic or probabilistic. It's important to note that all of these loss functions primarily target the predicted action, directly addressing step 2, with an implicit assumption of resolving step 1. This approach has exhibited effectiveness in simple scenarios where the robot has to learn a single task without variations \cite{zhang2018deep_vr_teleoperation,duan2017one_shot_il} and in multi-task settings where scenes are comprised of easily distinguishable objects \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt}. However, in complex scenarios composed of different distractor objects, the cognitive task becomes considerably more intricate and may be beyond resolution solely through the information derived from the action-based loss, increasing the gap that exists between the training metric and the actual goal (i.e., understand the command, genarete the action based on the current state and the command comprehension, and solve the desired task).
Starting from all the considerations above, the stated problem can be faced in two ways:
\begin{enumerate*}[label=(\arabic*)]
    \item Enhancing dataset diversity by expanding the representation of objects and employing a large-scale model capable of assimilating all the knowledge inherent in the dataset.
    \item Dividing the Decision-Making Problem into two primary blocks, namely a reasoning module and an action module. This segmentation enables the independent validation of each task within the decision-making process and the separate evaluation of the performance of each block. Subsequently, a method should be devised to integrate these two components, creating a modular system capable of solving complex problems. This system would consist of interconnected modules designed to address simpler problems.
\end{enumerate*}
In our second year of research, we opted to concentrate on a procedure aligned with the second approach. This choice was motivated by its effectiveness in assessing the various sub-tasks inherent in a decision-making problem, such as the control of a robotic system. Comprehensive details regarding the exact procedure and its formalization will be presented in the forthcoming Section \ref{sec:research_activity}.
