\section{Research Topic}
\label{sec:research_topic}
One of the primary objectives of robotics is to develop autonomous robots capable of performing manipulation tasks, such as pick-and-place or assembly operations. These tasks are prevalent in both industrial and social robotics domains. While traditional industrial operations can be addressed with hand-written controllers based on known objects and fixed actions, the current landscape of social robotics and modern industrial applications demands a greater level of adaptability and flexibility. These scenarios require robots to handle dynamic environments, recognize diverse objects, estimate their positions, and adapt their actions accordingly.
\newline In this scenario, traditional hand-written controllers are not well-suited, and there has been a notable shift towards data-driven approaches based on Machine Learning and Deep Learning algorithms over the past decade.
\newline Instead of relying on explicitly programmed rules and control algorithms, data-driven approaches leverage large amounts of data to enable robots to learn from examples and make intelligent decisions. Machine Learning techniques, such as supervised learning, unsupervised learning, and reinforcement learning, have become increasingly popular in robotics.
\newline In this context, this research activity aims to address the problem of Learning from Demonstration, where the goal is to obtain a policy, i.e., a function that maps a sequence of states into the corresponding sequence of actions, from a set of examples. Specifically, with respect to the general problem and the requirements given in Section \ref{sec:sota}, especially the requirement of high-adaptability, the goal is to design a \textbf{Vision-based Imitation Learning} system capable of learning the execution of \textbf{different tasks},  starting from a set of demonstrations. The resulting policy must be able to infer the current action from a visual observation of the environment from the robot's point of view. Such a system can find applications both in industrial (e.g., ask to a robot to pick a tool and then use it) and social robotic scenarios (e.g., ask to a housecare robot to move an object from one point to another), altough the main focus will be posed on Industrial tasks (e.g., pick-and-place, pushing, peg-insertion).
Among the four approaches presented in Section \ref{sec:sota} the one that is most suitable for the context of interest is \textit{Behavioral Cloning}, in particular those methods that leverage \textit{Meta-Learning} algorithms \cite{finn2017one_shot_visual_il,yu2018one_shot_hil,yu2018daml} or in general train a \textit{Multi-Task Imitation Learning} system \cite{jang2022bc_z,mandi2022towards_more_generalizable_one_shot}. From Table \ref{table:mosaic} it can be noted that in terms of pure success-rate both in the Single-Task and in the Multi-Task Setting the problem is far from being solved, and going from the first setting to the second one is no trivial. Indeed, also in the most ``simple" setting, i.e., when instances of the tested task are used during the training, the success rate decreases for almost all the tasks. These results points out the potential of the proposed multi-task imitation learning systems, since, as reported in the works \cite{jang2022bc_z,yu2018daml}, the main reason for the failure of these systems lies in errors in the identification of the critical points of a task and the subsequent execution of the correct action, instead of in the general identification of the intent of the task to be performed.
In addition to the overall assessment we conducted following the literature review, it is essential to emphasize that significant modifications have been made in relation to the Research Proposal presented at the conclusion of the first year. These alterations were prompted by interesting insights obtained from preliminary experiments, which subsequently enabled us to identify a previously unexplored research gap and formulate the ensuing research question:
\paragraph{\textit{Reason-then-act: How the separation between perception and action can affect the performance of an Imitation-Learning System}} \mbox{} \\
In both \textit{``Language-Conditioned Imitation Learning"} and \textit{``Video-Conditioned Imitation Learning"}, the primary objective is to obtain a function $\pi: S \times C \rightarrow A $, that can effectively map the current state and command pair to the corresponding action. These systems need to address two key challenges:
\begin{enumerate}
    \item \textbf{Generate Meaningful Semantic Information}: The first challenge involves the generation of meaningful semantic information from the command. This requires understanding the command's content, identifying relevant objects or actions mentioned, and encoding this information in a way that is useful for the task. In language-conditioned systems, natural language processing techniques, such as text parsing, semantic analysis, or neural language models \cite{stepputtis2020language,jang2022bc_z,brohan2022rt}, are commonly employed to extract and represent the semantics of the command. In image-conditioned systems, image processing and computer vision techniques (e.g., deep learning architectures \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}) are used to extract relevant features from the image query.
    \item \textbf{Fuse Information from Current State and Command}: The second challenge is to fuse the information extracted from the current observed state and the command in a meaningful way. This fusion aims to create an intermediate embedding or representation that captures the relevant aspects from both inputs. Techniques such as feature concatenation \cite{james2018task_embedded,stepputtis2020language,bhutani2022attentive_one_shot}, attention mechanisms \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}, or feature-wise linear modulation \cite{brohan2022rt} are utilized to combine the visual information from the state and the semantic information from the command. The fused representation should capture the important cues for inferring the correct action based on the current input and command.
\end{enumerate}
Despite the progress made with Multi-Task Imitation Learning, the performance of these agents still lacks consistent robustness when faced with challenges such as distractor objects and varying backgrounds \cite{brohan2022rt}. During our preliminary experimentation we tried to emphasize the first problem in order to \textbf{focus our attention to perception problems} rather than pure control problems, by testing the models in scenarios where there are objects that difference for very high-level features such as the color, and we observed how the system struggles to correctly identify the desired object but manages to successfully complete the task by placing the object in the appropriate bin, as it will explained in Chapter \ref{chapter:preliminary_results}.
\newline The observed results can be explained through a comprehensive analysis of the underlying learning procedure. Specifically, the reference methods \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt} are trained using the principles of Behavioral Cloning (BC). BC methodologies primarily concentrate on determining the appropriate action to execute within a given context,. This approach has exhibited effectiveness in single-task Imitation Learning scenarios \cite{zhang2018deep_vr_teleoperation,duan2017one_shot_il} and in multi-task settings where scenes are comprised of easily distinguishable objects, as demonstrated in \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot,brohan2022rt}. However, it is important to recognize that this simplistic approach may fail when used in challenging scenarios. In such situations, the system must not only determine the correct action but also identify the target object within the scene, exploiting a loss function that operates within a fundamentally different space, such as the action space. While this approach remains feasible in straightforward scenarios, where the network can effectively execute the correct action by leveraging the output of simple derivative filters, it may prove insufficient in complex scenarios where objects exhibit substantial variations in high-level features.
\newline In essence, the limitations of the BC-based training become apparent in complex scenarios, where the information gleaned from the BC loss function may prove to be inadequate for achieving desired performance outcomes. Consequently, it is imperative to explore more sophisticated and adaptable training approaches in order to address the challenges posed by complex scenarios.
Starting from all the consideration above, the stated problem can be faced in two ways:
\begin{enumerate}
    \item Enhancing dataset diversity by expanding the representation of objects and employing a large-scale model capable of assimilating all the knowledge inherent in the dataset.
    \item Dividing the Decision-Making Problem into two primary blocks, namely a reasoning module and an action module. This segmentation enables the independent validation of each task within the decision-making process and the separate evaluation of the performance of each block. Subsequently, a method should be devised to integrate these two components, creating a modular system capable of solving complex problems. This system would consist of interconnected modules designed to address simpler problems.
\end{enumerate}
In our second year of research, we opted to concentrate on a procedure aligned with the second approach. This choice was motivated by its effectiveness in assessing the various sub-tasks inherent in a decision-making problem, such as the control of a robotic system. Comprehensive details regarding the exact procedure and its formalization will be presented in the forthcoming Section \ref{sec:research_activity}.
% % Starting from this general consideration regarding the state of the art, the current research proposal aims to focus on two closely related aspects, which are described in the paragraph in the following paragraphs.
% \paragraph{\textit{What is the best representation of the task to facilitate the identification of critical points?}}  \mbox{} \\
% In the works currently proposed in the literature, the dataset is mainly composed of a combination of demonstration videos showing the robot task execution and the action performed by the robot itself, represented by the linear and angular velocity of the end-effector \cite{yu2018daml} or the desired pose of the end-effector \cite{dasari2021transformers_one_shot,jang2022bc_z}. Regarding the ability to identify critical points within the task, the criticality of the proposed solutions lies in the fact that the visual observations are composed of RGB images and recorded with respect to a third-person viewpoint \cite{yu2018daml,jang2022bc_z}. With this setting, at least two important critical issues are generated: The first is related to the lack of geometric information, e.g., the relative distance between the gripper and the object being approached. The second is related to the fact that the observations represent a different point of view from that of the robot, which can lead to the creation of occlusions generated by the movement of the arm itself, further complicating the detection of critical points such as the fact that the object is close and therefore the gripper can be closed. With respect to these problems, a possible solution could be to augment the observations by also adding a fourth component represented by the depth image, and also recording the movements from a robot's point of view, for example, by mounting a camera attached the gripper, as is usually done in Vision-Based grasping systems \cite{fang2020graspnet}.
% \newline Moreover, another important aspect related to the task representation is the lack of semantic information that can lead to a correct and meaningful task segmentation.
% It would be fascinating to obtain a system capable of automatically extracting subtasks from a set of demonstrations of different tasks composed of the same primitives, e.g., approach and push. Current methods \cite{jang2022bc_z,mandi2022towards_more_generalizable_one_shot} have demonstrated a limitation in solving this problem, evidenced especially by the performance decay in the multi-task setting compared to the single-task setting \cite{mandi2022towards_more_generalizable_one_shot,zhang2018deep_vr_teleoperation}, highlighting how training a monolithic architecture that does not explicitly consider task segmentation can lead to performance that is too low for real-world application. Work that has attempted to handle automatic task segmentation has been \cite{yu2018one_shot_hil,Mandlekar2020GTI}. Specifically, in \cite{yu2018one_shot_hil}, a task-phase predictor was trained on a set of primitive demonstrations (e.g., pushing, lifting, placing) based on their length to be used later as a sub-task selector. In \cite{Mandlekar2020GTI}, a goal-conditioned Variational-Autoencoder was trained to identify an efficient representation of the sub-task to be executed, to be used later for training a conditional policy on that representation. With respect to these methods, and taking inspiration from action-recognition-based methods \cite{goyal2017something2something}, an alternative approach may be based on using sub-task action labels, then training a multi-task architecture capable of generating in output both the action for task execution and a label related to the running sub-task that the robot is executing.

% \paragraph{\textit{What is the best architecture for modeling a policy?}}  \mbox{} \\
% A task demonstration extends over two dimensions, the first is the temporal one, and the second is the spatial one. Different types of architectures have been used in the context of Learning from Demonstration, starting from the classic Convolutional Neural Network \cite{zhang2018deep_vr_teleoperation,yu2018daml,yu2018one_shot_hil} up to the modern architectures based on Transformers \cite{dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}. From different results reported in \cite{mandi2022towards_more_generalizable_one_shot,mandlekar2022matters}, it was possible to observe how the use of architectures capable of managing time sequences is crucial to obtain a system with a high success rate. In the context of multi-task learning, however, an additional level must be added, linked to the representation of the tasks and their respective sub-tasks. It is reasonable to think that having an architecture capable of obtaining a representation linked to the sub-tasks performed in the different tasks may lead to greater generalization across the tasks. The architecture closest to this modeling is \cite{Mandlekar2020GTI}, which uses a conditional Variational Autoencoder to learn a representation of the sub-task starting from the current observation and the observation representative of the final state. In this paper, however, the visual appearance of the proposed tasks was the same, an assumption that may be too strong in a more general context, such as that proposed by \cite{mandi2022towards_more_generalizable_one_shot}. A possible improvement concerning this setting can be represented by the use of Contrastive Loss, used in \cite{sermanet2018time_contrastive,zakka2022xirl}, in order to support the generation of a representation as similar as possible for the same sub-tasks but coming from different contexts and as different as possible for different sub-tasks