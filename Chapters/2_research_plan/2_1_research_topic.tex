\section{Research topic}
\label{sec:research_topic}
Regardless of the application area, whether industrial or social, one of the main goals of robotics is to have a system capable of performing various tasks in dynamic environments characterized by a high degree of uncertainty. This system must be characterized by several properties, among which are high reliability in execution, high adaptability to new scenarios and operating conditions, and high simplicity in robot programming. As seen from the study of the literature, learning-based methods appear to be of interest, given their ability to infer a task execution policy from a series of demonstrations, mainly when the application context is characterized by high uncertainty, such as the location or type of the target object or the task itself that is to be executed. Although the problem of Learning from Demonstration has been studied for many decades, there are still substantial gaps at different levels of abstraction. Gaps can be identified that are both methodological and purely applicative. In fact, regarding the former, there are different questions that, despite being studied, have not yet been fully answered. These gaps will be proposed and briefly discussed below.

\paragraph{\textit{How can the compounding-error be mitigated?}} As reported in Section \ref{sec:sota}, the compounding-error is the most important limitation related to Imitation Learning. Solutions proposed in the literature are mainly based on algorithms that require active interaction between the learner and the demonstrator. This may lead to both safety, and data-efficiency problems for complex long-horizons tasks. While in the context of autonomous vehicle driving is prohibitive and very risky to generate bad examples, in the context of robotic manipulation, it may have sense to produce a set of wrong demonstrations (e.g., demonstrations where the robot fails to pick the target object), quantitatively evaluate the quality of a demonstration, and show recovery behaviors. A possible trace for improvements can be represented  

\paragraph{\textit{How can a system generalize both to different instances of the same task and to different tasks?}} When it comes to systems based on Artificial Intelligence techniques, the leading property of this should be the generalization capability. In applying such systems in the context of task control and execution, there are at least two levels of generalization. The first is a single-task generalization capability, and the second is a multi-task generalization capability. The former is related to the desire to obtain a system that can correctly execute different instances of the same task (e.g., different objects and/or different initial conditions). The latter, on the other hand, is related to the desire to have a system able to solve different tasks without complete retraining, which can be time-consuming and data inefficient. To achieve this goal, recent methods proposed to use Meta-Learning algorithms, showing promising results but with relatively low overall performance. A possible trace for improvements can be represented by a better exploitation of temporal information. In particular, given the relatively low ammount of data in the produced dataset, the exploitation of temporal information should be combined with efficient and meaningful task representation, given by latent-variable in Model-Based methods.

\paragraph{\textit{How can last-centimeter errors be mitigated?}} As stated in the paragraph dedicated to Behavioral Cloning (pag. \pageref{sec:lfo}), one of the main reasons for task failure is related to errors in perception and identification of key points during task execution (e.g., failure to close the gripper on object approach, hitting the object of interest because the system does not recognize its presence). A possible way to solve these problems is to use a multi-modal perception system, which could include tactile and/or proximity information in addition to RGB(-D) visual information, in order to explicitly model, in the demonstrations, the fact that the robot touches an object then it closes the gripper. Moreover, the idea behind Hierarchical Task Decomposition methods may also play an important role in solving such problems since they model sub-policy activation/deactivation when a given event is detected, however the event detection should not based on hand-written conditions (e.g., distance between objects computed through the identification of specific markers), but the system should be able to learn, and then produce automatically the sub-tasks relationships. 

\paragraph{\textit{How can a human describe the task to be executed?}} Being able to express the intent as efficiently and naturally as possible can lead to several benefits, both in terms of pure human-robot interaction (e.g., during the system operation, the human can describe the task to execute using a voice command), and during the training procedure itself. Regarding this last point, the human intent can be described by means of video demonstrations or voice command. These can be useful for two reasons, \begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item in a multi-task setting, natural language description and/or human video can be used as policy conditioning;
    \item in the LfO approach, the whole dataset is composed of only unlabeled video of human performing a task.
\end{enumerate*}

As for the questions more related to application aspects. Based on the proposed state of the art, there are a few that tower above all others, mainly related to the approaches of IRL, GAIL, and LfO, and they are: 
\begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item \textit{Can these approaches be effectively used for solving complex manipulation tasks on real robotic platforms?}
    \item \textit{A system obtained through such an approach, what generalization capability does it have?}
\end{enumerate*} In particular, the theoretical properties behind training frameworks related to IRL and GAIL are well known. In addition, proposed experiments both in simulation and real environments on more or less complex tasks have shown promising results, especially for methods based on GAIL and LfO. However, on the basis of current knowledge, there is a lack of experimental analyses in real environments that can point out flaws of such approaches when applied in a real context and thus guide the development of new methods to solve them. For example, most LfO methods applied in robot manipulation are based on some image-to-image translation, where the video of a human demonstration is translated into the corresponding robot video by means of complex generative architecture, but, instead of using the video demonstration as it is, \textit{can a more efficient and effective task representation be obtained?} Such representation could be an object centric trajectory extrapolated from the human demonstration, or a set of keypoints tracked from the human arm movements. In this way, the problem becomes either to directly find the robot action that generates the desired object movement, or find the mapping between human and robot motions.