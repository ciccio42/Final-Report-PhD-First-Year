\paragraph{Behavioral Cloning (BC).} \mbox{} \\
Behavioral Cloning is one of the first approach used to learn a task from a set of demonstrations. Generally, it is framed as a Supervised Learning problem. Algorithm \ref{alg:bc} defines the classic procedure used to solve the learning task, where, given the dataset $\mathcal{D}^{E}$, a parameterized learner policy $\pi^{L}_{\theta}$, and a loss-function $\mathcal{L}$, the goal is to find the policy's parameter that minimizes the loss-function, or in other terms, $\theta^{*} = \underset{\theta}{argmin} \ \mathbb{E}_{(\boldsymbol{\tau}, c) \sim \mathcal{D}^{E}} \ [\mathcal{L}((\boldsymbol{\tau}, c), \ \pi^{L}_{\theta})]$.
In this setting there are at least two questions to answer:
\begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item choose whether to optimize in trajectory space or action space;
    \item what kind of representation to use for the policy.
\end{enumerate*}
\newline According to \cite{osa2018algorithmic}, BC methods can be categorized as Model-Free and Model-Based. Moreover, a further classification can be based on the fact that the policy is optimized either in the \textit{trajectory space} or in the \textit{action space}.
%Generally speaking, the most used methods are the model-free methods since, in the context of Behavioral Cloning for Robot Task Learning and Control, the basic assumption is the access to the ground truth state and action, so the problem becomes to mimic the demonstrated behavior. While Model-Based becomes of interest when 
\input{Algorithm/algorithm_bc}

\textbf{Model-Free methods} learn a policy that reproduces the expert's behavior without learning/estimating system dynamic. Since they do not require neither to estimate the dynamic nor to learn a reward function, they are simple to implement and do not necessary require interactions with the environment during the learning procedure.

Model-Free methods that derive policy in the trajectory space were very popular in the context of Model-Free BC for trajectory planning, given their ability to explicitly model constraints (e.g. a smooth convergence to the goal state). In this setting well studied methods are the \textit{Dynamic Movement Primitives} (DMPs) \cite{ijspeert2002learning,ijspeert2013dynamical}, which can represent non-linear movements without losing the stability of the behavior.
Authors in \cite{ijspeert2013dynamical} formulated the Dynamic Movement Primitive for a single DoF point-to-point trajectory using the following set of non-linear differential equations: \input{Formula/dmp.tex} Where $\beta_{s}, \alpha_{s}, \alpha_{z}$ are constants, $s$ is the system state, $z$ is the phase-variable function of time $t$, and $f$ is the forcing-term which describes the trajectory non-linear behavior. Generally, $f$ is a linear combination of basis-function $\psi_{i}(z)$ (e.g., Gaussian basis function), $f(z(t)) = (g-s_{0}) \sum_{i=1}^{M}\psi_{i}(z(t))\omega_{i}z$. Basically, a DMP describes a point-attractor system, where the current system state $s$ must converge to the goal state $g$, starting from $s_{0}$. In this setting, the aim is to learn the set of weights, $\left\{\omega_{i}, i=1,\dots,M\right\}$, which can be obtained by solving a supervised learning problem with loss function $\mathcal{L}_{DMP} = \sum_{t=0}^{T}(f_{target}(t) - f(z(t)))^{2}$, where $f_{target}(t) = \tau^{2} \ \ddot{s}^{E}_{t} - \beta_{s}(\alpha_{s}(g - s^{E}_{t})-\tau \dot{s}^{E}_{t})$. DMPs formulation has been used in the context of robotic manipulation. For example in \cite{meier2011movement_primitive,caccavale2019kinesthetic,agostini2020manipulation} the problem of task decomposition was faced, and DMPs have been used to model the sub-tasks. However, DMPs formulation has a series of problems related to: \begin{enumerate*}[label=(\textbf{\alph*})]
    \item how to handle stochasticity in demonstrations;
    \item how to explicitly define basis function
    given the desired behavior;
    \item how to handle arbitrary desired trajectories
    with intermediate via-points;
    \item how to handle complex high-dimensional inputs.
\end{enumerate*}
For all the above mentioned problems some solutions have been proposed. For example, the stochasticity problem was addressed in \cite{paraschos2013ProMPs}, where the Probabilistic Movement Primitives (ProMPs) method was proposed. In ProMPs the probability of observing a trajectory $\boldsymbol{\tau}$ is written as $\boldsymbol{\tau} = \underset{t}{\prod} \ \mathcal{N}(s(t)|\Psi(z(t))^{T}\omega, \Sigma_{s})$, where $\Psi$ is a time-dependent basis matrix. As in DPMs, the goal is to find the weights $\omega$ by solving a supervised learning problem. As for the second problem, other trajectory representations have been proposed based on Hidden Markov Model, Gaussian Mixture Models, Kernelized Movement Primitives. However, all these alternatives scale bad when the goal is to learn a policy starting from visual observations. To handle such complex high-dimensional input Deep Architecture have been proposed, which will be explained in detail in the next sections.
%As for the second problem, the idea proposed in the literature is to use deep architecture to learn the DMPs parameters \cite{ridge2020training_nn_dmps}. 
\newline Regarding the methods that derive the policy in the action-space. One of the primal work was \cite{pomerleau1988alvinn}, which proposed \textit{ALVINN}, an autonomous vehicle driving system based on a Neural Network, that infers the steering angle, given a synthetic camera image as input. The network was trained on pairs (image, steering-angle) and the training procedure was defined as a supervised classification problem, since the steering-angle was discretized over 45 units. This work immediately emphasized the problem of compounding-error, caused by \textbf{covariate-shift phenomena}. This issue occurs because an action $a_{t}$ influences the next state $s_{t+1}$, which represents the next sample, violating the i.i.d assumption of Supervised Learning and generating a test-data distribution, that may be different from the training one. This phenomena has a relevant consequence on the expected performance of the system. Indeed, assuming to have a system that makes an error with probability $\epsilon$, and a task with time-horizon $T$, then, due to compounding error, a supervised learner reaches a total cost of $O(\epsilon \ T^{2})$, rather than $O(\epsilon \ T)$ \cite{ross2010efficient_reductions,ross2011dagger}. To attenuate this problem, interactive supervised learning algorithms have been proposed, such as the well-known \textit{DAgger} \cite{ross2011dagger}. Algorithm \ref{alg:dagger} describes the DAgger procedure. It is an aggregation strategy, based on the idea to train the policy $\pi^{L}$ under the state-distribution induced by the policy itself, but with the correct action performed by the expert. The main problem with DAgger is that it requires the expert to interact with the system during the training, introducing both safety and data-efficiency problems, especially when the system does not provide the human expert with sufficient control authority during the sampling process \cite{laskey2017comparing_hc_rc}. \input{Algorithm/dagger}
\newline Human-Guided DAgger (HG-DAgger) \cite{kelly2019hg_dagger} is an extension of the classic DAgger strategy, in which the human expert observes the rollout of the current policy, so if the agent has entered an unsafe region of the state space, the expert takes control and guides the system to a safe and stable region. In \cite{jang2022bc_z} was shown how HG-DAgger can be effectively used in the context of robotic manipulation. Indeed, starting from the same total number of episodes, a policy trained with only expert demonstration has a significantly lower success rate than a policy trained on a dataset with both expert demonstrations and expert adjustments. In the context of Interactive Learning for Robot Manipulation, other works of interest include \cite{mandlekar2020human_in_the_loop,chisari2022correct}. In \cite{chisari2022correct}, a human expert provides both \textbf{corrective} and \textbf{evaluative} feedback. %(Figure \ref{fig:feedback}).
The former consists in the human that takes control of the robot to adjust the trajectory, the latter consists in a scalar weight $q$, set to 1 if the trajectory is satisfactory, 0 if the trajectory is not satisfactory, $\alpha$ if the trajectory is adjusted by the expert, where $\alpha$ is the ratio between non-corrected and corrected samples. Then a Neural Network % in Figure \ref{fig:architecture}
was trained by minimizing a weighted version of the maximum-likelihood, $\mathcal{L}(a_{t},s_{t}) = - q \ log(\pi^{L}_{\theta}(a_{t}|s_{t}))$. Real-world experiments show that with a training time of \textbf{41 minutes}, including environmental reset, it was possible to have an agent capable of performing tasks such as picking up a cube or pulling a plug.
%\input{Figures/correct-me-if-i-am-wrong}
%Classic Behavioral Cloning

Despite the covariate-shift problem, \cite{zhang2018deep_vr_teleoperation} showed that very interesting performance can be obtained in the context of Robot Manipulation, by means of Behavioral Cloning and high quality demonstrations given by teleportation system. In this work, a Convolutional Neural Network was trained to predict the desired linear-velocity, angular-velocity of the end-effector, and the binary gripper state (open/close), given in input the current RGB-D observation of the scene, and the position of three points of the end-effector, during the last 5 time-steps (Figure \ref{fig:deep_bc}). The system was tested on 10 tasks, and the performance are reported in Table \ref{table:deep_vr_teleoperation_results}. The proposed system achieved a high success rate while evaluating all the tasks. The tests were carried out from different initial conditions but still quite similar to those present in the training set (e.g., the initial object positions have been uniformly distributed within the training regime, with random local variations around these positions). The analysis of failure cases showed that the leading cause of errors was the inability to detect critical points in the task execution, such as closing/opening the gripper to pick/place the object or detect the position of the object of interest in order to avoid collision with it.\input{Figures/deep_visual_bc.tex}\input{Table/deep_bc_teleoperation_results}\unskip Another important aspect to note is that, for each task, the network was trained from scratch. This is not a desired property for a highly adaptable system, as stated in Section \ref{sec:intro}. For this reason, methods based on Meta-Learning algorithms have been proposed.

\textbf{Meta-Learning} is based on the idea to train a model on a variety of tasks, in such a way that it can solve a new task, using only a small number of training samples \cite{finn2017maml}. Algorithm \ref{alg:maml} describes the steps followed by the \textit{Model-Agnostic Meta-Learning} (MAML) algorithm \cite{finn2017maml}, that is the base for different methods which apply One-shot Imitation Learning in the context of Behavioral Cloning \cite{finn2017one_shot_visual_il,yu2018daml,yu2018one_shot_hil}.\input{Algorithm/maml}
\newline In \cite{finn2017one_shot_visual_il}, MAML algorithm was used to prove the effectiveness of Meta-Learning in the context of real robot manipulation, with visual observations, as opposite to \cite{duan2017one_shot_il}. A Convolutional Neural Network was trained by following the Algorithm \ref{alg:maml}, using as loss-function the Mean Squared Error, computed between the predicted action and the ground truth one. For real-robot experiments a dataset of \textbf{1300} placing demonstrations (i.e., place an holded object in a target container), containing near to \textbf{100} different objects, was collected through teleportation. The trained system was tested by performing the adaptation step on one video demonstration, over 29 new objects, moreover, between the video demonstration and the actual execution, the objects configuration was changed. In this setting the system reached the $\mathbf{90\%}$ of success rate, outperforming baseline methods based on LSTM \cite{duan2017one_shot_il}, and contextual network (i.e., a Convolutional Neural Network that takes in input the current observation and the image representing the target state).
In \cite{yu2018daml}, the \textit{Domain Adaptive Meta-Learning} algorithm (DAML) was proposed with the goal of learning to infer a policy from a single human demonstration. To achieve it, a two-step algorithm was proposed. In the first-step, called \textbf{Meta-Learning step}, given in input, for each task $\mathcal{T}$, a set of human demo $\mathcal{D}^{h}_{\mathcal{T}}$ and a set or robot demo $\mathcal{D}^{r}_{\mathcal{T}}$ (Figure \ref{fig:daml}), the \textit{initial policy parameters} $\theta$ and the \textit{adaptive loss} parameters $\psi$ are learned, solving the problem in Formula \ref{eq:daml}. \input{Formula/daml}
\newline Where the outer loss is $\mathcal{L}_{BC}(\phi,\mathbf{d^{r}}) = \sum_{t} log(\pi_{\phi}(a_{t}|s_{t},o_{t}))$, and the inner-loss $\mathcal{L}_{\psi}$, is the learned adaptive loss, which is used during the \textbf{Meta-Test step}, where the policy parameters are adapted with gradient descent given in input a video of human demonstrating a new task $\mathcal{T}$, i.e., $\phi_{\mathcal{T}} = \theta - \alpha \nabla_{\theta} \mathcal{L}_{\psi}(\theta, \mathbf{d}^{h})$. Experimental evaluation on tasks such as placing, pushing, and pick-and-place, has shown that: \begin{enumerate*}[label=\textbf{(\alph*)}]
    \item the system was able to generalize across both new objects and objects configuration starting from only a single human-demonstration;
    \item a performance degradation was observed in large domain-shift experiments, such as novel backgrounds and different camera view-points.
\end{enumerate*}
\input{Figures/daml}
Meta-Learning algorithms have demonstrated intriguing properties, notably their capacity for few-shot generalization to novel objects and object configurations. However, it has been observed that during the adaptation step, these methods tend to lose their effectiveness in performing other tasks. This limitation has underscored the need for the development of Multi-Task Imitation Learning methods, which aim to address these shortcomings and enable more versatile task execution in complex scenarios.

\textbf{Multi-Task Imitation Learning} aims to solve the problems related to Meta Learning. Indeed, the final goal of such methods is to obtain a general-purpose robot that is able to master a wide range of tasks and quickly learn a novel one by leveraging past experience.
\newline Indeed, starting from an \textit{expert dataset} containing $n$ tasks, $\mathcal{D}^{E}=\left \{\mathcal{D}_{1}, \dots, \mathcal{D}_{n}\right \}$, where $\mathcal{D}_{i} = \left \{ (\tau_{m_{i}}^{j}, \ c_{m_{i}}), j=1,\dots,N, \ m_{i} \in \mathcal{M}_{i}\right \}$ is the \textit{single-task dataset}, composed of:
\begin{itemize}
    \item $N$ expert demonstration for each $m^{th}$ variation of the $i^{th}$ task (Figure \ref{fig:mosaic_tasks}). $M_{i}$ is the number of variation for the $i^{th}$ task.
    \item Agent trajectories $\tau_{m_{i}}^{j} = (s_{0}, a_{0}, s_{1}, a_{1}, \dots, a_{T-1}, s_{T})$. where $s_{t}$ is the state at time $t$ and $a_{t}$ is the corresponding action (Section \ref{sec:problem_formulation}).
    \item Task-conditioning $c_{m_{i}}$ for the $m^{th}$ variation of $i^{th}$ task, which describes the desired task in terms of video demonstrations \cite{james2018task_embedded,bhutani2022attentive_one_shot,dasari2021transformers_one_shot,mandi2022towards_more_generalizable_one_shot}, natural language description \cite{stepputtis2020language,jang2022bc_z,mees2022calvin,doasIcan2022,mees2022hulc,brohan2022rt,shridhar2023perceiver} or multi-modal prompt, that exploits both visual information (e.g., an image of the target object) and text information that contains the information related to the action to be performed \cite{jiang2023vima}.
\end{itemize}
The goal of Multi-Task Imitation Learning is to learn a \textit{conditioned policy} $\pi^{L}_{\theta}(a_{t}|s_{t}, c_{m_{i}})$, that is able to map the current state and command into the corresponding action.
Depending on how the policy is defined, various loss functions come into play. In the case of deterministic policies, the learning process focuses on minimizing the Mean-Squared Error (refer to Formula \ref{eq:mse}). However, for probabilistic policies, the learning process centers around minimizing the Negative Log-likelihood (refer to Formula \ref{eq:nll}). This approach aims to enhance the probability of correctly executing the action.
\input{Formula/loss_functions.tex}
\newline Regarding the methods that use a selection of frames as conditioning signal, there are approaches such as \cite{james2018task_embedded,bhutani2022attentive_one_shot} that use the first and last frame of the task demonstration. The fundamental concept underlying these methods involves the utilization of a Deep Convolutional Neural Network to encode this task representation. Subsequently, the resultant embedding serves as input for the control network, as illustrated in Figure \ref{fig:task_embedded}. It is worth noting that these approaches tend to face a significant challenge, namely, that the conditioning signal fails to adequately encapsulate pertinent information regarding the optimal approach for solving the task. This limitation arises from the fact that the conditioning signal lacks comprehensive encoding of task-solving strategies.
\input{Figures/task_embedded.tex}
\newline The methodologies introduced in \cite{bhutani2022attentive_one_shot} and \cite{mandi2022towards_more_generalizable_one_shot} aim to advance the concept of an ideal multi-task agent, capable of replicating a new task based on a single demonstration, often performed by another agent (e.g., a robot or a human). The task execution takes place in a setting that differs from the one in which the demonstrator operated, potentially involving distinct object configurations, as illustrated in Figure \ref{fig:mosaic}. One particularly noteworthy contribution is presented in \cite{mandi2022towards_more_generalizable_one_shot}, where the authors introduce the Multi-task One-Shot imitation with self-Attention and Contrastive Learning (MOSAIC) architecture. This architecture is designed to model a demonstration-conditioned policy, denoted as $\pi^{L}_{\theta}(a_{t}|s_{t},c)$, with $c$ representing the current task demonstration, such as a video depicting a robot performing the task. MOSAIC addresses the challenge of multi-task imitation learning by incorporating two key components:
\begin{enumerate*}[label=\textbf{(\arabic*)}]
    \item A time-contrastive loss, serving as additional supervision for representation learning, with the goal of obtaining similar embeddings for temporally proximate frames;
    \item A self-attention model architecture, employed to extract contextual information from demonstrations, which is then utilized by the neural network to infer actions.
\end{enumerate*}
To facilitate experimentation and evaluation, a dataset containing \textbf{seven distinct tasks} and \textbf{61 variations} (Figure \ref{fig:mosaic_tasks}) was generated by executing hand-written policies in a simulation environment. This dataset was employed to train and compare the proposed MOSAIC architecture against other one-shot (meta-)imitation learning methods, such as \cite{yu2018daml} and \cite{dasari2021transformers_one_shot}. The results, presented in Table \ref{table:mosaic}, demonstrate that MOSAIC surpasses previous methods in the Single-Task One-shot Imitation Learning Setting, specifically when addressing individual tasks with multiple variations and testing on previously unseen scenarios (i.e., novel objects configuration).
Furthermore, when subjected to evaluation in a Multi-Task setting, wherein the system is trained on all the tasks and subsequently tested on each task separately, MOSAIC exhibits the capability to partially replicate the demonstrated tasks. It is important to note that transitioning from a single-task to a multi-task context presents inherent challenges. Despite being familiar with the tasks used during training, the success rate tends to decrease for almost all tasks. This phenomenon underscores the necessity for both training procedures and architectures capable of generating embeddings that accurately represent both the task itself and its various sub-tasks. This capacity is essential for reusing these embeddings when executing new instances or entirely novel tasks.
\input{Figures/mosaic.tex}
\input{Table/mosaic_results.tex}
In addition to the previously discussed visual-conditioned techniques, the scientific literature has also explored an alternative approach to define a conditioning signal. This alternative method involves employing a \textit{\textbf{natural language description}} of the task that is intended for execution. This approach enables a human operator to communicate with the robot, specifying the task it should perform. The inception of these methods was prompted by the realization that a natural language command, such as \textit{``Pick the blue cube and place it in the red bowl"}, not only conveys information about the desired task (i.e., picking and placing) but also contains semantic information about the two target objects (i.e., the blue cube and the red bowl). Consequently, through training a neural network with a diverse set of tasks, the system should exhibit the ability to generalize its understanding to both previously unseen objects within familiar tasks and entirely novel tasks composed of the fundamental actions practiced during training. This approach showcases the potential for robust and adaptable human-robot interaction in real-world scenarios.
\newline Foundational research, exemplified by works such as \cite{stepputtis2020language} and \cite{jang2022bc_z}, has sought to investigate the previously stated hypothesis. In particular, the authors of \cite{stepputtis2020language} introduced an innovative architectural framework, illustrated in Figure \ref{fig:language_conditioned}, marking the first instance where language, vision, and control tasks were seamlessly integrated. This model consists of two essential components: a Semantic Model, responsible for generating a task embedding denoted as $e$, using the initial scene image and the accompanying text command, and a Control Model, tasked with generating the control signal, utilizing the current robot state $r_{t}$ and the task embedding $e$.
Training of this model was conducted on two fundamental tasks, namely ``Picking" and ``Pouring", within scenarios featuring multiple objects of the same category, which served as distractors (see Figure \ref{fig:objects}). The subsequent testing experiments demonstrated the system's capability to successfully complete the picking task 98 out of 100 times and the pouring task 85 out of 100 times within novel scenarios. These results serve as compelling evidence of the efficacy and potential of language-conditioned methodologies in the field.
\input{Figures/language_conditioned.tex}
Authors in \cite{jang2022bc_z} make a step towards a more general agent, by proposing a large-scale dataset containing \textbf{100} diverse manipulation tasks. The demonstrations were collected through both expert teleoperation and shared autonomy process (HG-DAgger \cite{kelly2019hg_dagger}). The demonstrated tasks were related to pick-and-place, grasp, pick-and-drag, pick-and-wipe, and push skills. The dataset was used to train the network in Figure \ref{fig:bcz_architecture}. As it can be noted the samples were composed by the current robot observation, and a conditioning represented by either a \textit{\textbf{natural language description}} or a \textit{\textbf{human video demonstration}}.
% The idea was that training a conditioned policy over the current observation $o_{t}$, and a task representation $c$, , it would allow the policy to generalize over new tasks in a zero-shot manner (i.e., without any fine-tuning). 
Experimental results shown that, over 28 held-out tasks, containing both completely new objects, and known objects but in different tasks, an average success rate of \textbf{38\%} was reached in the easiest setting, with only one distractor and with natural language instruction. The success rate dropped to \textbf{4\%} in the hardest setting with 4 distractors and video conditioning.
\input{Figures/bc-z}
\newline Foundational research in the field of Language-Conditioned Multi-Task Imitation Learning has demonstrated promising results in zero-shot generalization. However, the robustness of their performance remains a challenge. Subsequent research, as highlighted in \cite{brohan2022rt,mees2022calvin,mees2022hulc}, has focused on enhancing performance. In particular, the authors of \cite{brohan2022rt} sought to investigate whether the transfer of knowledge from extensive, diverse, and task-agnostic datasets, which has enabled modern machine learning models to excel in zero or few-shot learning scenarios for new and specific tasks, is applicable within the realm of robotics. This inquiry arises due to the presence of high-capacity architectures capable of assimilating knowledge from such large datasets. To explore this prospect, the authors in \cite{brohan2022rt} introduced a comprehensive dataset comprising over 130,000 demonstrations collected across more than 700 household tasks (as depicted in Figure \ref{fig:rt_1_dataset}). Additionally, they proposed a Language-Conditioned Transformer-based architecture (as illustrated in Figure \ref{fig:rt_1_model}).
\input{Figures/rt1_model.tex}
\input{Table/rt_1_dataset.tex}
It is worth noting the intriguing findings presented in Table \ref{table:rt1_results}. The model exhibits robustness in replicating tasks it has encountered before, and it even performs reasonably well on tasks it hasn't seen previously. However, a notable decline in performance becomes apparent when the model is exposed to novel backgrounds and scenarios featuring distracting objects, especially when the available data for these situations is limited. This observed trend holds significance because, unlike domains such as Computer Vision and Natural Language Processing where gathering large-scale datasets is relatively straightforward, the collection of real-world robotic datasets is a laborious and time-consuming endeavor. Furthermore, these real-world robotic datasets often have limited applicability to other research due to \textbf{disparities in action space, robot morphology, and scene representation}, as pointed out by \cite{brohan2022rt}.Therefore, the aspiration is to develop a system capable of replicating tasks with minimal demonstrations specifically gathered from the particular robot in use.
\newline Furthermore, the decline in success performance when faced with distractor objects emphasizes that addressing the policy-learning problem in an end-to-end manner, which involves mapping high-dimensional and high-level inputs like images and text directly to low-dimensional, low-level outputs such as the actions to be executed, may not be the most effective approach. This is because such models might lack the necessary perceptual components that enable them to initially recognize the target object within the scene, subsequently navigate towards it, and commence the manipulation process. This process aligns with how humans approach manipulation tasks \cite{grill2003neural}.
\input{Table/rt_1_results.tex}
%These bad performance can be justified by the classic training procedure followed, while different advantages may be gained from Meta-Learning algorithms, and by how the human video embedding was obtained, through a CNN on a 4x5 matrix of images, with a completely loss of temporal information.
% In their analysis, the authors pointed out that the system did not fail to identify the object, but in what they called ``\textbf{last-centimeter errors}", i.e., failing to close the gripper, failing to let go of objects, or a near miss of the target object when letting go of an object in the gripper.

The results reported by the different methods pointed out how there is a wide room for improvement in the context of both Single-Task and Multi-Task Few-shot Imitation Learning methods, which are particularly promising when it comes to task-level generalization, because the intent of the new task is correctly captured, especially in the case of conditioned policies. However, the success rate in task execution is relatively low for several reasons that can range from difficulty in identifying key points in task execution (e.g., the robot has approached the object and, therefore, the gripper must be closed) to compounding error, which leads to new observations different from those in the training set (e.g., the robot did not grasp the object correctly). As a result, the robot cannot complete the task because it performs actions inconsistent with the state of the task itself. Possible improvements can range from using a multi-modal system based on visual and tactile information in order to make explicit the transition between consecutive task phases, to adding bad demonstrations combined with recovery behavior during data collection.

The methods presented up to now partially solve the problems related to compounding error and task-generalization. Another relevant problem related to Behavioral Cloning, and more in general to Learning from Demonstration, is the execution of \textbf{multi-stage long-horizon tasks}. Indeed, it is pretty intuitive to think that a manipulation task is characterized by a modular structure which can be exploited to improve performance, following the idea that each sub-task can be learned more efficiently because each skill is shorter-horizon. In this setting, the main question to answer is: \textit{How can a task be decomposed into the corresponding component skills?} In its classical formulation, this problem can be framed as a classification problem. In a preliminary work \cite{meier2011movement_primitive}, the authors used a set of predefined primitive motions modeled as DMPs. The goal was to recognize these primitives within the demonstrated trajectory using an Expectation-Maximization algorithm. Also, recent methods \cite{caccavale2019kinesthetic,agostini2020manipulation} exploit the DMPs formulation. However, they are not part of the process of task segmentation. For example, in \cite{caccavale2019kinesthetic}, the action segmentation is performed based on either object proximity or explicit human command. When the robot manager identifies a new action, the corresponding DMP is learned, and the sequence of tasks is organized in a hierarchical structure. The problem of task-segmentation was also solved starting from an unconstrained video demonstrations labeled with commands \cite{xu2018neural_task_programming}, activity \cite{yang2015robot}, or completely unlabeled \cite{yu2018one_shot_hil,Mandlekar2020GTI}. In \cite{yu2018one_shot_hil}, the authors proposed an extension of DAML for multi-stage tasks. The main idea of this work was based on modeling the distribution of tasks, $p(\mathcal{T})$ as a set of primitives (e.g., reach primitive). During the meta-training phase, two systems were obtained: \begin{enumerate*}[label=(\textbf{\arabic*})]
    \item a human/robot-phase predictor, whose responsibility was to indicate the completion rate of the current primitive given a human video demonstration and a robot video demonstration, respectively;
    \item a Convolutional Neural Network trained according to DAML, which acted as a motion-primitive meta-model policy, parameterized by meta-parameters $\theta$.
\end{enumerate*} During the meta-test phase, given as input a video of human performing a compound task, three steps were defined: \begin{enumerate*}[label=(\textbf{\arabic*})]
    \item extrapolate the set of primitives by using the human-phase predictors;
    \item for each primitive, starting from the meta-parameters $\theta$, find the parameters $\phi_{i}$ by performing gradient descent on the learned loss function $\mathcal{L}_{\psi}$;
    \item execute the policy parameterized by $\phi_{i}$, until the current sub-task is completed (completion detected by the robot-phase predictor).
\end{enumerate*} The work proposed in \cite{Mandlekar2020GTI} addressed the multi-stage long-horizon tasks with a different approach. The authors did not focus on an explicit concept of task segmentation. Indeed, they started with the idea that similar tasks intersect at common regions of the state-space, and proposed a two-stage IL architecture able to exploit this intersecting structure to generalize to unseen start-goal state combinations.
\newline These methods are interesting because they allow the execution of a compound task from simple unlabeled videos, thus with as little effort as possible in dataset generation. However, since they do not have semantic information about which part of the task is being executed, they may err on the side of interpretability of actions, which is not the case with methods based on labeled videos or hierarchical structures.

\textbf{Model-Based methods} are characterized by the fact that during the learning process the system dynamic model is learned. There are several reasons why it may be necessary to use the dynamic system model in the context of Learning from Demonstration, for example: \begin{enumerate*}[label=\textbf{(\alph*)}]
    \item a difference in the embodiment between the demonstrator and the learner;
    \item a difference in the task execution conditions and parameters.
\end{enumerate*}
Among these methods, the most important aspect is related to how the dynamic model is retrieved, e.g., Gaussian Mixture Model \cite{grimes2009learning_actions_through_imitation} or Gaussian Process \cite{englert2013probabilistic,deisenroth2014multi_task}. In recent years, a significant amount of attention has been placed on systems that can replicate a task from a video of human performing a task without any access to ground-truth action. While in the survey mentioned before, these methods were still considered Model-Based Behavioral Cloning. In the current literature, given the increasing amount of works, they can now belong to a new category called Learning from Observation. Methods belonging to this category will be explained in the specific paragraph.