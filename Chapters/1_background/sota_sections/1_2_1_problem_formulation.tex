\subsection{Problem Definition}
\label{sec:problem_formulation}
Imitation Learning aims to obtain an agent that can replicate the behavior demonstrated by an expert agent. The behavior is described by a \textbf{policy}, where $\pi^{L}$ defines the learned policy, and $\pi^{E}$ defines the expert one. In the broadest possible definition, $\pi^{L}$ is obtained starting from a dataset $\mathcal{D}=\left \{ \left ( \boldsymbol{\tau}_{i}, \boldsymbol{c}_{i}\right ) \right \}_{i}^{N}$, where:
\begin{itemize}
    \item $\boldsymbol{\tau}_{i}$ is the $i^{th}$ demonstrated trajectory, which can be described as:
          \begin{itemize}
              \item A state-action sequence, i.e., $\boldsymbol{\tau}_{i} = [s_{0}, a_{0}, \dots, s_{T}, a_{T}]$, when it is assumed to have access to the ground-truth action performed by the expert.
              \item A state sequence, i.e., $\boldsymbol{\tau}_{i} = [s_{0}, \dots, s_{T}]$, when it is assumed to not have access to the ground-truth action.
          \end{itemize}
    \item $\boldsymbol{c}_{i}$ is the \textit{context-vector}, it contains task-related information, e.g. the initial state of the system $s_{0}$, the position of the target object, or a representation of the task to be executed (e.g., natural language description of the task or video demonstrations).
\end{itemize}
The state $s_{i}$ can be represented by combination of images representing the robot's workspace and robot's proprioceptive information such as joints position, velocities and torques. While the ground truth action can be obtained by teleoperation or kinesthetic teaching (Section \ref{sec:source_of_demonstration}).
\newline The policy $\pi^{L}$ can be defined with respect to different abstraction levels \cite{fang2019survey,osa2018algorithmic}:
\begin{enumerate*}[label=(\textbf{\alph*})]
    \item \textit{Symbolic Characterization}, the policy maps states, and context to a sequence of options, i.e., $\pi: s_{t}, \textbf{c} \rightarrow [o_1, \dots, o_T]$, where each option is a sequence of actions. With this representation, complex tasks can be decomposed into a sequence of simple movements. However, it is hard to achieve an accurate task segmentation and motion ordering;
    \item \textit{Trajectory Characterization}, the policy maps context to trajectory, i.e., $\pi: \mathbf{c} \rightarrow \boldsymbol{\tau}$. Because it allows the initial state to be mapped to a complete sequence of actions, this representation can be used to obtain the options in the Symbolic Representation. However, they need as many dynamic features as possible, that can be difficult to obtain;
    \item \textit{State-Action Characterization}, the policy maps states(-context) to actions, i.e., $\pi: s_{t}, \textbf{c} \rightarrow a_{t}$. This representation makes it possible to map the current state directly to the corresponding action. However, it is easy for errors to accumulate in long-term processes.
\end{enumerate*}
\newline The agent, by means of its policy $\pi$, acts on a system, which is modeled as a \textit{Markov Decision Process} (MDP) \cite{kroemer2021review_robot_learning}. A MDP is defined as a tuple $(S,A,R,T,\gamma)$, where:
\begin{itemize}
    \item $S \subseteq \mathbb{R}^{n}$, is a set of states (e.g. joints position and/or image).
    \item $A \subseteq \mathbb{R}^{n}$, is a set of actions, (e.g. desired end-effector pose, desired joints torque).
    \item $R(s,a,s')$, is a \textit{reward function}, which expresses the immediate reward for executing action $a$ in state $s$, and transitioning to state $s'$.
    \item $T(s'|s,a)$ is a \textit{transition function}, which defines the probability to reach the state $s'$, after the execution of action $a$ in state $s$. This distribution, which describe the \textit{system dynamic}, can be given a priori or learned (Model-Based methods), or do not taken into account (Model-Free methods).
    \item $\gamma \in [0,1]$, is a discount factor expressing the agent's preference for immediate over future rewards.
\end{itemize}
With respect to the given MDP definition, the reward function plays different roles based on the used approach. Indeed, in BC methods, the reward function is not implicitly used, but a surrogate loss-function is used instead. In IRL the reward function is learned, assuming that the expert acts (near-)optimal w.r.t. some unknown reward function. In GAIL and LfO, the rule played by the reward function is different based on the specific method, as will be explained in Section \ref{sec:methods}.